{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c057f2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import html\n",
    "import re\n",
    "import numpy\n",
    "import sys\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from math import floor, ceil\n",
    "#from gensim.models import KeyedVectors\n",
    "#from gensim.downloader import load\n",
    "\n",
    "#model = load('word2vec-google-news-300')\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self, metric_dict):\n",
    "        #save paths to clean, dirty and corrected dataset\n",
    "        self.clean_path = metric_dict[\"clean_data_path\"]\n",
    "        self.dirty_path = metric_dict[\"dirty_data_path\"]\n",
    "        #self.corrected_path = metric_dict[\"corrected_data_path\"]\n",
    "        \n",
    "        #read csv-files of clean, dirty and corrected dataset\n",
    "        self.clean_data = self.read_csv_dataset(metric_dict[\"clean_data_path\"])\n",
    "        self.dirty_data = self.read_csv_dataset(metric_dict[\"dirty_data_path\"])\n",
    "        self.corrected_data = self.read_csv_dataset(metric_dict[\"corrected_data_path\"])\n",
    "        \n",
    "        #create dictionaries for errorneous cells\n",
    "        #save clean, dirty and corrected values of erroneous cells seperately\n",
    "        self.error_clean_val = self.get_dataframes_difference(self.dirty_data, self.clean_data) #clean values\n",
    "        self.error_dirty_val = self.get_dataframes_difference(self.clean_data, self.dirty_data) #dirty values\n",
    "        self.error_corrected_val = self.get_error_corrected_val() #corrected values\n",
    "        \n",
    "        #save attribute classification\n",
    "        self.str_attr = metric_dict[\"str_attr\"]\n",
    "        self.short_str_attr = metric_dict[\"short_str_attr\"]\n",
    "        self.long_str_attr = metric_dict[\"long_str_attr\"]\n",
    "        self.numer_attr = metric_dict[\"numer_attr\"]\n",
    "        \n",
    "        #calculate \"normal\" metrics precision, recall, f1\n",
    "        self.standard_metric = self.get_data_cleaning_evaluation()\n",
    "        \n",
    "        #numeric TP\n",
    "        self.numer_tp = self.get_numer_tp()\n",
    "        \n",
    "        #calculate and save fuzzy metrics\n",
    "        #self.fuzzy_alt_metric = self.get_data_cleaning_evaluation_fuzzy_alt()\n",
    "        self.fuzzy_jw = self.get_data_cleaning_evaluation_fuzzy_JW()\n",
    "        self.fuzzy_me = self.get_data_cleaning_evaluation_fuzzy_ME()\n",
    "        #self.fuzzy_ld_words = self.get_data_cleaning_evaluation_fuzzy_LD_Words() \n",
    "        self.fuzzy_ld_char = self.get_data_cleaning_evaluation_fuzzy_LD_Char() \n",
    "        #self.fuzzy_ld = self.get_data_cleaning_evaluation_fuzzy_LD() if self.short_str_attr or self.long_str_attr else {\"LD Message\": \"short or long string attributes not declared\"}\n",
    "        self.fuzzy_semantics_sentences = self.get_data_cleaning_evaluation_fuzzy_semantic_sentences()\n",
    "        \n",
    "        #fuzzy metrics combined with outlier metric\n",
    "        if self.numer_attr:\n",
    "            #self.fuzzy_alt_num_metric = self.get_data_cleaning_evaluation_fuzzy_alt(True)\n",
    "            self.fuzzy_jw_num = self.get_data_cleaning_evaluation_fuzzy_JW(True)\n",
    "            self.fuzzy_me_num = self.get_data_cleaning_evaluation_fuzzy_ME(True)\n",
    "            #self.fuzzy_ld_words_num = self.get_data_cleaning_evaluation_fuzzy_LD_Words(True)\n",
    "            self.fuzzy_ld_char_num = self.get_data_cleaning_evaluation_fuzzy_LD_Char(True)\n",
    "            #self.fuzzy_ld_num = self.get_data_cleaning_evaluation_fuzzy_LD(True) if self.short_str_attr or self.long_str_attr else {\"LD Num Message\": \"short or long string attributes not declared\"}\n",
    "            self.fuzzy_semantics_sentences_num = self.get_data_cleaning_evaluation_fuzzy_semantic_sentences(True)\n",
    "            \n",
    "        #average metrics\n",
    "        self.avg_string_metric = self.get_string_metric_avg()\n",
    "        self.avg_string_semantic_metric = self.get_string_semantic_metric_avg()\n",
    "        \n",
    "        #combined evaluation\n",
    "        self.combined_metric = self.get_combined_score_evaluation()\n",
    "\n",
    "        \n",
    "    def print_metrics(self):\n",
    "        print(self.combined_metric)\n",
    "        print(\"\")\n",
    "        print(self.standard_metric)\n",
    "        #print(self.fuzzy_alt_metric)\n",
    "        print(self.fuzzy_jw)\n",
    "        print(self.fuzzy_me)\n",
    "        #print(self.fuzzy_ld_words)\n",
    "        print(self.fuzzy_ld_char)\n",
    "        #print(self.fuzzy_ld)\n",
    "        #print(self.fuzzy_semantics_words)\n",
    "        print(self.fuzzy_semantics_sentences)\n",
    "        print(\"\")\n",
    "        \n",
    "        if self.numer_attr:\n",
    "            #print(self.fuzzy_alt_num_metric)\n",
    "            print(self.fuzzy_jw_num)\n",
    "            print(self.fuzzy_me_num)\n",
    "            #print(self.fuzzy_ld_words_num)\n",
    "            print(self.fuzzy_ld_char_num)\n",
    "            #print(self.fuzzy_ld_num)\n",
    "            #print(self.fuzzy_semantics_words_num)\n",
    "            print(self.fuzzy_semantics_sentences_num)\n",
    "            \n",
    "        else:\n",
    "            print({\"Num Metrics\": \"numeric attributes not declared\"})\n",
    "\n",
    "        \n",
    "    def read_csv_dataset(self, dataset_path):\n",
    "        \"\"\"\n",
    "        This method reads a dataset from a csv file path.\n",
    "        \"\"\"\n",
    "        dataframe = pandas.read_csv(dataset_path, sep=\",\", header=\"infer\", encoding=\"utf-8\", dtype=str,\n",
    "                                    keep_default_na=False, low_memory=False).applymap(self.value_normalizer)\n",
    "        return dataframe\n",
    "    \n",
    "    @staticmethod\n",
    "    def value_normalizer(value):\n",
    "        \"\"\"\n",
    "        This method takes a value and minimally normalizes it.\n",
    "        \"\"\"\n",
    "        value = html.unescape(value)\n",
    "        value = re.sub(\"[\\t\\n ]+\", \" \", value, re.UNICODE)\n",
    "        value = value.strip(\"\\t\\n \")\n",
    "        return value\n",
    "    \n",
    "    def get_string_metric_avg(self):\n",
    "        if True:\n",
    "            ld_words_char_avg_p = list(self.fuzzy_ld_char.values())[0]\n",
    "            ld_words_char_avg_r = list(self.fuzzy_ld_char.values())[1]\n",
    "            ld_words_char_avg_f1 = list(self.fuzzy_ld_char.values())[2]\n",
    "            avg_precision = (list(self.fuzzy_jw.values()))[0] + (list(self.fuzzy_me.values()))[0] +  ld_words_char_avg_p / 3\n",
    "            avg_recall = (list(self.fuzzy_jw.values()))[1] + (list(self.fuzzy_me.values()))[1] +  ld_words_char_avg_r / 3\n",
    "            avg_f1 = (list(self.fuzzy_jw.values()))[2] + (list(self.fuzzy_me.values()))[2] +  ld_words_char_avg_f1 / 3\n",
    "        \n",
    "        else:\n",
    "            avg_precision = (list(self.fuzzy_jw.values())[0] + list(self.fuzzy_me.values())[0] + list(self.fuzzy_ld.values())[0])/3\n",
    "            avg_recall = (list(self.fuzzy_jw.values())[1] + list(self.fuzzy_me.values())[1] + list(self.fuzzy_ld.values())[1])/3\n",
    "            avg_f1 = (list(self.fuzzy_jw.values())[2] + list(self.fuzzy_me.values())[2] + list(self.fuzzy_ld.values())[0])/3\n",
    "\n",
    "        return {\"Average of String Metrics Precision\": round(avg_precision,3), \"Average of String Metrics Recall\": round(avg_recall,3), \"Average of String Metrics F1\": round(avg_f1,3)}\n",
    "        \n",
    "    def get_string_semantic_metric_avg(self):\n",
    "        avg_precision = (list(self.avg_string_metric.values())[0] + list(self.fuzzy_semantics_sentences.values())[0]) / 2\n",
    "        avg_recall = (list(self.avg_string_metric.values())[1] + list(self.fuzzy_semantics_sentences.values())[1]) / 2\n",
    "        avg_f1 = (list(self.avg_string_metric.values())[2] + list(self.fuzzy_semantics_sentences.values())[2]) / 2\n",
    "        \n",
    "        return {\"Average of String and Semantics Metrics Precision\": round(avg_precision,3), \"Average of String and Semantics Metrics Recall\": round(avg_recall,3), \"Average of String and Semantics Metrics F1\": round(avg_f1,3)}\n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_dataframes_difference(self, dataframe_1, dataframe_2):\n",
    "        \"\"\"\n",
    "        This method compares two dataframes and returns the different cells.\n",
    "        \"\"\"\n",
    "        if dataframe_1.shape != dataframe_2.shape:\n",
    "            sys.stderr.write(\"Two compared datasets do not have equal sizes!\\n\")\n",
    "        difference_dictionary = {}\n",
    "        difference_dataframe = dataframe_1.where(dataframe_1.values != dataframe_2.values).notna()\n",
    "        for j in range(dataframe_1.shape[1]):\n",
    "            for i in difference_dataframe.index[difference_dataframe.iloc[:, j]].tolist():\n",
    "                difference_dictionary[(i, j)] = dataframe_2.iloc[i, j]\n",
    "        return difference_dictionary\n",
    "    \n",
    "    def get_error_corrected_val(self):\n",
    "        correction_dict = self.get_dataframes_difference(self.dirty_data, self.corrected_data)\n",
    "        for key in list(correction_dict):\n",
    "            if key not in self.error_clean_val:\n",
    "                del correction_dict[key]\n",
    "    \n",
    "        return correction_dict\n",
    "    \n",
    "  \n",
    "    def jaro_winkler_distance(self, s1, s2):\n",
    "        \"\"\"\n",
    "        Compute Jaro-Winkler distance between two strings.\n",
    "        \"\"\"\n",
    "        # If the s are equal\n",
    "        if (s1 == s2):\n",
    "            return 1.0\n",
    "\n",
    "        # Length of two s\n",
    "        len1 = len(s1)\n",
    "        len2 = len(s2)\n",
    "\n",
    "        # Maximum distance upto which matching\n",
    "        # is allowed\n",
    "        max_dist = floor(max(len1, len2) / 2) - 1\n",
    "\n",
    "        # Count of matches\n",
    "        match = 0\n",
    "\n",
    "        # Hash for matches\n",
    "        hash_s1 = [0] * len(s1)\n",
    "        hash_s2 = [0] * len(s2)\n",
    "\n",
    "        # Traverse through the first\n",
    "        for i in range(len1):\n",
    "\n",
    "            # Check if there is any matches\n",
    "            for j in range(max(0, i - max_dist), \n",
    "                           min(len2, i + max_dist + 1)):\n",
    "\n",
    "                # If there is a match\n",
    "                if (s1[i] == s2[j] and hash_s2[j] == 0):\n",
    "                    hash_s1[i] = 1\n",
    "                    hash_s2[j] = 1\n",
    "                    match += 1\n",
    "                    break\n",
    "\n",
    "        # If there is no match\n",
    "        if (match == 0):\n",
    "            return 0.0\n",
    "\n",
    "        # Number of transpositions\n",
    "        t = 0\n",
    "        point = 0\n",
    "\n",
    "        # Count number of occurrences\n",
    "        # where two characters match but\n",
    "        # there is a third matched character\n",
    "        # in between the indices\n",
    "        for i in range(len1):\n",
    "            if (hash_s1[i]):\n",
    "\n",
    "                # Find the next matched character\n",
    "                # in second\n",
    "                while (hash_s2[point] == 0):\n",
    "                    point += 1\n",
    "\n",
    "                if (s1[i] != s2[point]):\n",
    "                    t += 1\n",
    "                point += 1\n",
    "        t = t//2\n",
    "\n",
    "        # Return the Jaro Similarity\n",
    "        return (match/ len1 + match / len2 +\n",
    "                (match - t) / match)/ 3.0\n",
    "    \n",
    "\n",
    "    def jaro_winkler_distance_fuzzy(self, clean, dirty, corrected):\n",
    "        jw_clean_dirty = self.jaro_winkler_distance(clean, dirty)\n",
    "        jw_clean_corrected = self.jaro_winkler_distance(clean, corrected)\n",
    "\n",
    "\n",
    "        return jw_clean_corrected - jw_clean_dirty\n",
    "    \n",
    "    \n",
    "    def get_data_cleaning_evaluation_fuzzy_JW(self, num_metric=False):\n",
    "        \"\"\"\n",
    "        This method evaluates data cleaning process using fuzzy metrics\n",
    "        \"\"\"\n",
    "        pc_r = 0.0 #right partial corrections\n",
    "        pc_f = 0.0 #false partial corrections\n",
    "        ec_tp = 0.0\n",
    "        output_size = 0.0\n",
    "        \n",
    "        for cell in self.error_corrected_val:\n",
    "            output_size += 1\n",
    "            if self.error_corrected_val[cell] == self.error_clean_val[cell]:\n",
    "                ec_tp += 1.0\n",
    "            elif (num_metric and cell[1] in self.str_attr and cell[1] not in self.numer_attr) or (not num_metric and cell[1] in self.str_attr):\n",
    "                metric_score = self.jaro_winkler_distance_fuzzy(self.error_clean_val[cell], self.error_dirty_val[cell],  self.error_corrected_val[cell])\n",
    "                ec_tp += metric_score\n",
    "                print([self.error_clean_val[cell] ,  self.error_dirty_val[cell],  self.error_corrected_val[cell], metric_score])\n",
    "                if metric_score >= 0:\n",
    "                    pc_r += 1\n",
    "                else:\n",
    "                    pc_f += 1 \n",
    "        \n",
    "        \n",
    "        if num_metric:\n",
    "            ec_tp += self.numer_tp\n",
    "            ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "            ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "            ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "            return {\"Fuzzy JW Num Precision\": round(ec_p, 3),\"Fuzzy JW Num Recall\": round(ec_r, 3), \"Fuzzy JW Num F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "        else:\n",
    "            ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "            ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "            ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "            return {\"Fuzzy JW Precision\": round(ec_p, 3),\"Fuzzy JW Recall\": round(ec_r, 3), \"Fuzzy JW F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "\n",
    "    def get_data_cleaning_evaluation(self):\n",
    "        \"\"\"\n",
    "        This method evaluates data cleaning process using fuzzy metrics\n",
    "        \"\"\"\n",
    "        ec_tp = 0.0\n",
    "        output_size = 0.0\n",
    "        for cell in self.error_corrected_val:\n",
    "            output_size += 1\n",
    "            if self.error_corrected_val[cell] == self.error_clean_val[cell]:\n",
    "                ec_tp += 1.0\n",
    "        ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "        ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "        ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "        return {\"Precision\": round(ec_p, 3),\"Recall\": round(ec_r, 3), \"F1\": round(ec_f, 3), \"Amount of fixed data errors\": output_size}\n",
    "    \n",
    "    def get_single_outlier_score(self, clean, dirty, corrected):\n",
    "        score = 1 - (abs(int(clean)-int(corrected)) / abs(int(clean) - int(dirty)))\n",
    "    \n",
    "        if score >= 0:\n",
    "            return score\n",
    "\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    def get_fuzzy_score_outlier(self, clean, dirty, corrected):\n",
    "        clean = re.findall(r'\\d+', clean)\n",
    "        dirty = re.findall(r'\\d+', dirty)\n",
    "        corrected = re.findall(r'\\d+', corrected)\n",
    "\n",
    "        if len(clean) != len(dirty) or len(dirty) != len(corrected) or len(corrected) != len(clean):\n",
    "            return 0\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for (o,d,c) in  zip(clean, dirty, corrected):\n",
    "            count += self.get_single_outlier_score(o, d, c)\n",
    "\n",
    "        return count/len(clean)\n",
    "    \n",
    "    def get_numer_tp(self):\n",
    "        \"\"\"\n",
    "        This method evaluates data cleaning process using fuzzy metrics\n",
    "        \"\"\"\n",
    "\n",
    "        ec_tp = 0.0\n",
    "        for cell in self.error_corrected_val:\n",
    "            if cell in self.error_clean_val:\n",
    "                if cell[1] in self.numer_attr and self.error_corrected_val[cell] != self.error_clean_val[cell]:\n",
    "                    ec_tp += self.get_fuzzy_score_outlier(self.error_clean_val[cell], self.error_dirty_val[cell],  self.error_corrected_val[cell])\n",
    "                    \n",
    "        return ec_tp \n",
    "    \n",
    "    \n",
    "    def get_data_cleaning_evaluation_fuzzy_alt(self, num_metric=False):\n",
    "        \"\"\"\n",
    "        This method evaluates data cleaning process using fuzzy metrics\n",
    "        \"\"\"\n",
    "\n",
    "        ed_tp = 0.0\n",
    "        ec_tp = 0.0\n",
    "        output_size = 0.0\n",
    "        for cell in self.error_corrected_val:\n",
    "            output_size += 1\n",
    "            if cell in self.error_clean_val:\n",
    "                ed_tp += 1.0\n",
    "                if self.error_corrected_val[cell] == self.error_clean_val[cell]:\n",
    "                    ec_tp += 1.0\n",
    "                elif cell[1] in self.numer_attr and num_metric:\n",
    "                    ec_tp += self.get_fuzzy_score_outlier(self.error_clean_val[cell], self.error_dirty_val[cell],  self.error_corrected_val[cell])\n",
    "                elif cell[1] in self.str_attr:\n",
    "                    ec_tp += self.get_fuzzy_score_string_alt(self.error_corrected_val[cell], self.error_clean_val[cell], self.error_corrected_val[cell])\n",
    "        ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "        ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "        ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "        \n",
    "        if num_metric:\n",
    "            return {\"Fuzzy Alt Num Precision\": round(ec_p, 3),\"Fuzzy Alt Num Recall\": round(ec_r, 3), \"Fuzzy Alt Num F1\": round(ec_f, 3)}\n",
    "        else:\n",
    "            return {\"Fuzzy Alt Precision\": round(ec_p, 3),\"Fuzzy Alt Recall\": round(ec_r, 3), \"Fuzzy Alt F1\": round(ec_f, 3)}\n",
    "    \n",
    "    def get_fuzzy_score_string_alt(self, clean, dirty, corrected ):\n",
    "        if len(clean) != len(corrected) or len(dirty) != len(clean):\n",
    "            return 0\n",
    "\n",
    "        count_w = 0.0\n",
    "        count_r = 0.0\n",
    "        for o, c, d in zip(clean, corrected, dirty):\n",
    "            if o != d:\n",
    "                count_w += 1\n",
    "                if o == c:\n",
    "                    count_r += 1\n",
    "            else:\n",
    "                if c != d:\n",
    "                    count_r -= 1\n",
    "\n",
    "        if count_r <= 0:\n",
    "            return 0\n",
    "\n",
    "        return count_r / count_w\n",
    "    \n",
    "    def get_data_cleaning_evaluation_fuzzy_ME(self, num_metric=False):\n",
    "        \"\"\"\n",
    "        This method evaluates data cleaning process using fuzzy metrics\n",
    "        \"\"\"\n",
    "        pc_r = 0.0 #right partial corrections\n",
    "        pc_f = 0.0 #false partial corrections\n",
    "        \n",
    "        ec_tp = 0.0\n",
    "        output_size = 0.0\n",
    "        for cell in self.error_corrected_val:\n",
    "            output_size += 1\n",
    "            if cell in self.error_clean_val:\n",
    "                if self.error_corrected_val[cell] == self.error_clean_val[cell]:\n",
    "                    ec_tp += 1.0\n",
    "                elif (num_metric and cell[1] in self.str_attr and cell[1] not in self.numer_attr) or (not num_metric and cell[1] in self.str_attr):\n",
    "                    metric_score = self.monge_elkan_distance_fuzzy([self.error_corrected_val[cell]], [self.error_clean_val[cell]], [self.error_dirty_val[cell]])\n",
    "                    ec_tp += metric_score\n",
    "                    if metric_score >= 0:\n",
    "                        pc_r += 1\n",
    "                    else:\n",
    "                        pc_f += 1\n",
    "    \n",
    "        if num_metric:\n",
    "            ec_tp += self.numer_tp\n",
    "            ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "            ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "            ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "            return {\"Fuzzy ME Num Precision\": round(ec_p, 3),\"Fuzzy ME Num Recall\": round(ec_r, 3), \"Fuzzy ME Num F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "        else:\n",
    "            ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "            ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "            ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "            return {\"Fuzzy ME Precision\": round(ec_p, 3),\"Fuzzy ME Recall\": round(ec_r, 3), \"Fuzzy ME F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def monge_elkan(self, bag1, bag2):\n",
    "        \"\"\"\n",
    "        Compute Monge-Elkan similarity measure between two bags (lists).\n",
    "\n",
    "        The Monge-Elkan similarity measure is a type of Hybrid similarity measure that combine the benefits of\n",
    "        sequence-based and set-based methods. This can be effective for domains in which more control is needed\n",
    "        over the similarity measure. It implicitly uses a secondary similarity measure, such as levenshtein to compute\n",
    "        over all similarity score.\n",
    "\n",
    "        Args:\n",
    "            bag1,bag2 (list): Input lists\n",
    "\n",
    "            sim_func (function): Secondary similarity function. This is expected to be a sequence-based\n",
    "                similarity measure (defaults to levenshtein)\n",
    "\n",
    "        Returns:\n",
    "            Monge-Elkan similarity score (float)\n",
    "\n",
    "        Raises:\n",
    "            TypeError : If the inputs are not lists or if one of the inputs is None\n",
    "\n",
    "\n",
    "        Examples:\n",
    "            >>> monge_elkan(['Niall'], ['Neal'])\n",
    "            0.8049999999999999\n",
    "            >>> monge_elkan(['Comput.', 'Sci.', 'and', 'Eng.', 'Dept.,', 'University', 'of', 'California,', 'San', 'Diego'], ['Department', 'of', 'Computer', 'Science,', 'Univ.', 'Calif.,', 'San', 'Diego'])\n",
    "            0.8677218614718616\n",
    "            >>> monge_elkan(['Comput.', 'Sci.', 'and', 'Eng.', 'Dept.,', 'University', 'of', 'California,', 'San', 'Diego'], ['Department', 'of', 'Computer', 'Science,', 'Univ.', 'Calif.,', 'San', 'Diego'], sim_func=needleman_wunsch)\n",
    "            2.0\n",
    "            >>> monge_elkan(['Comput.', 'Sci.', 'and', 'Eng.', 'Dept.,', 'University', 'of', 'California,', 'San', 'Diego'], ['Department', 'of', 'Computer', 'Science,', 'Univ.', 'Calif.,', 'San', 'Diego'], sim_func=affine)\n",
    "            2.25\n",
    "            >>> monge_elkan([''], ['a'])\n",
    "            0.0\n",
    "            >>> monge_elkan(['Niall'], ['Nigel'])\n",
    "            0.7866666666666667\n",
    "\n",
    "        References:\n",
    "            * Principles of Data Integration book\n",
    "        \"\"\"\n",
    "\n",
    "        # if exact match return 1.0\n",
    "        if bag1 == bag2:\n",
    "            return 1.0\n",
    "        # if one of the strings is empty return 0\n",
    "        if (len(bag1) == 0) or (len(bag2) == 0):\n",
    "            return 0\n",
    "        # aggregated sum of all the max sim score of all the elements in bag1\n",
    "        # with elements in bag2\n",
    "        sum_of_maxes = 0\n",
    "        for t1 in bag1:\n",
    "            max_sim = float('-inf')\n",
    "            for t2 in bag2:\n",
    "                max_sim = max(max_sim, self.jaro_winkler_distance(t1, t2))\n",
    "            sum_of_maxes += max_sim\n",
    "        sim = float(sum_of_maxes) / float(len(bag1))\n",
    "        return 1 - sim\n",
    "\n",
    "\n",
    "    def monge_elkan_distance_fuzzy(self, clean, dirty, corrected):\n",
    "        me_clean_dirty = self.monge_elkan(clean, dirty)\n",
    "        me_clean_corrected = self.monge_elkan(clean, corrected)\n",
    "        #print(monge_elkan(clean, dirty))\n",
    "        #print(monge_elkan(clean, corrected))\n",
    "\n",
    "\n",
    "        return me_clean_corrected - me_clean_dirty\n",
    "        \n",
    "    def get_data_cleaning_evaluation_fuzzy_LD(self, num_metric=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        This method evaluates data cleaning process using fuzzy metrics\n",
    "        \"\"\"\n",
    "        pc_r = 0.0 #right partial corrections\n",
    "        pc_f = 0.0 #false partial corrections\n",
    "        ed_tp = 0.0\n",
    "        ec_tp = 0.0\n",
    "        output_size = 0.0\n",
    "        for cell in self.error_corrected_val:\n",
    "            output_size += 1\n",
    "            if cell in self.error_clean_val:\n",
    "                ed_tp += 1.0\n",
    "                if self.error_corrected_val[cell] == self.error_clean_val[cell]:\n",
    "                    ec_tp += 1.0\n",
    "                    \n",
    "                elif cell[1] in self.numer_attr and num_metric:\n",
    "                    ec_tp += self.get_fuzzy_score_outlier(self.error_clean_val[cell], self.error_dirty_val[cell],  self.error_corrected_val[cell])\n",
    "                    \n",
    "                elif cell[1] in self.long_str_attr:\n",
    "                    metric_score = self.fuzzy_LD_Words(self.error_clean_val[cell], self.error_dirty_val[cell],  self.error_corrected_val[cell])       \n",
    "                    ec_tp += metric_score\n",
    "                    if metric_score >= 0:\n",
    "                        pc_r += 1\n",
    "                    else:\n",
    "                        pc_f += 1\n",
    "                        \n",
    "                elif cell[1] in self.short_str_attr:\n",
    "                    metric_score = self.fuzzy_LD_Char(self.error_clean_val[cell], self.error_dirty_val[cell],  self.error_corrected_val[cell])\n",
    "                    ec_tp += metric_score\n",
    "                    if metric_score >= 0:\n",
    "                        pc_r += 1\n",
    "                    else:\n",
    "                        pc_f += 1\n",
    "                        \n",
    "        ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "        ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "        ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "        \n",
    "        if num_metric:\n",
    "            return {\"Fuzzy LD Num Precision\": round(ec_p, 3),\"Fuzzy LD Recall\": round(ec_r, 3), \"Fuzzy LD F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "        else:\n",
    "            return {\"Fuzzy LD Precision\": round(ec_p, 3),\"Fuzzy LD Recall\": round(ec_r, 3), \"Fuzzy LD F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "    \n",
    "\n",
    "    def get_data_cleaning_evaluation_fuzzy_LD_Char(self, num_metric=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        This method evaluates data cleaning process using fuzzy metrics\n",
    "        \"\"\"\n",
    "        pc_r = 0.0 #right partial corrections\n",
    "        pc_f = 0.0 #false partial corrections\n",
    "        \n",
    "        ec_tp = 0.0\n",
    "        output_size = 0.0\n",
    "        for cell in self.error_corrected_val:\n",
    "            output_size += 1\n",
    "            if self.error_corrected_val[cell] == self.error_clean_val[cell]:\n",
    "                ec_tp += 1.0\n",
    "            elif (num_metric and cell[1] in self.str_attr and cell[1] not in self.numer_attr) or (not num_metric and cell[1] in self.str_attr):\n",
    "                metric_score = self.fuzzy_LD_Char(self.error_clean_val[cell], self.error_dirty_val[cell],  self.error_corrected_val[cell])\n",
    "                ec_tp += metric_score\n",
    "                #print([self.error_clean_val[cell] ,  self.error_dirty_val[cell],  self.error_corrected_val[cell], metric_score])\n",
    "                if metric_score >= 0:\n",
    "                    pc_r += 1\n",
    "                else:\n",
    "                    pc_f += 1\n",
    "        ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "        ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "        ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "        \n",
    "        if num_metric:\n",
    "            ec_tp += self.numer_tp\n",
    "            ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "            ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "            ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "            return {\"Fuzzy LD Char Num Precision\": round(ec_p, 3),\"Fuzzy LD Char Num Recall\": round(ec_r, 3), \"Fuzzy LD Char Num F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "        else: \n",
    "            ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "            ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "            ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "            return {\"Fuzzy LD Char Precision\": round(ec_p, 3),\"Fuzzy LD Char Recall\": round(ec_r, 3), \"Fuzzy LD Char F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "    \n",
    "    def get_data_cleaning_evaluation_fuzzy_LD_Words(self, num_metric=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        This method evaluates data cleaning process using fuzzy metrics\n",
    "        \"\"\"\n",
    "        pc_r = 0.0 #right partial corrections\n",
    "        pc_f = 0.0 #false partial corrections\n",
    "        ed_tp = 0.0\n",
    "        ec_tp = 0.0\n",
    "        output_size = 0.0\n",
    "        for cell in self.error_corrected_val:\n",
    "            output_size += 1\n",
    "            if cell in self.error_clean_val:\n",
    "                ed_tp += 1.0\n",
    "                if self.error_corrected_val[cell] == self.error_clean_val[cell]:\n",
    "                    ec_tp += 1.0\n",
    "                elif (num_metric and cell[1] in self.str_attr and cell[1] not in self.numer_attr) or (not num_metric and cell[1] in self.str_attr):\n",
    "                    metric_score = self.fuzzy_LD_Words(self.error_clean_val[cell], self.error_dirty_val[cell],  self.error_corrected_val[cell])       \n",
    "                    ec_tp += metric_score\n",
    "                    if metric_score >= 0:\n",
    "                        pc_r += 1\n",
    "                    else:\n",
    "                        pc_f += 1\n",
    "        \n",
    "        if num_metric:\n",
    "            ec_tp += self.numer_tp\n",
    "            ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "            ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "            ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "            return {\"Fuzzy LD Words Num Precision\": round(ec_p, 3),\"Fuzzy LD Words Num Recall\": round(ec_r, 3), \"Fuzzy LD Words Num F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "        else:\n",
    "            ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "            ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "            ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "            return {\"Fuzzy LD Words Precision\": round(ec_p, 3),\"Fuzzy LD Words Recall\": round(ec_r, 3), \"Fuzzy LD Words F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "\n",
    "\n",
    "    def fuzzy_LD_Char(self, clean, dirty, corrected):\n",
    "        LD_clean_dirty =  self.levenshteinDistanceChar(clean, dirty)\n",
    "        LD_clean_corrected = self.levenshteinDistanceChar(clean, corrected)\n",
    "        print([clean, dirty, corrected, LD_clean_dirty, LD_clean_corrected,(LD_clean_corrected-LD_clean_dirty)])\n",
    "\n",
    "        return (LD_clean_corrected-LD_clean_dirty)\n",
    "    \n",
    "    \n",
    "    def fuzzy_LD_Words(self, clean, dirty, corrected):\n",
    "        LD_clean_dirty =  self.levenshteinDistanceWords(clean, dirty)\n",
    "        LD_clean_corrected = self.levenshteinDistanceWords(clean, corrected, (LD_clean_dirty - LD_clean_corrected) / max([len(clean), len(dirty)]))\n",
    "\n",
    "       \n",
    "        return (LD_clean_dirty - LD_clean_corrected) / max([len(clean), len(dirty)])\n",
    "        \n",
    "    def levenshteinDistanceWords(self, token1, token2):\n",
    "        token1 = token1.split()\n",
    "        token2 = token2.split()\n",
    "        distances = numpy.zeros((len(token1) + 1, len(token2) + 1))\n",
    "\n",
    "        for t1 in range(len(token1) + 1):\n",
    "            distances[t1][0] = t1\n",
    "\n",
    "        for t2 in range(len(token2) + 1):\n",
    "            distances[0][t2] = t2\n",
    "\n",
    "        a = 0\n",
    "        b = 0\n",
    "        c = 0\n",
    "\n",
    "        for t1 in range(1, len(token1) + 1):\n",
    "            for t2 in range(1, len(token2) + 1):\n",
    "                if (token1[t1-1] == token2[t2-1]):\n",
    "                    distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "                else:\n",
    "                    a = distances[t1][t2 - 1]\n",
    "                    b = distances[t1 - 1][t2]\n",
    "                    c = distances[t1 - 1][t2 - 1]\n",
    "\n",
    "                    if (a <= b and a <= c):\n",
    "                        distances[t1][t2] = a + 1\n",
    "                    elif (b <= a and b <= c):\n",
    "                        distances[t1][t2] = b + 1\n",
    "                    else:\n",
    "                        distances[t1][t2] = c + 1\n",
    "\n",
    "\n",
    "        return distances[len(token1)][len(token2)]\n",
    "\n",
    "    def levenshteinDistanceChar(self, token1, token2):\n",
    "        distances = numpy.zeros((len(token1) + 1, len(token2) + 1))\n",
    "\n",
    "        for t1 in range(len(token1) + 1):\n",
    "            distances[t1][0] = t1\n",
    "\n",
    "        for t2 in range(len(token2) + 1):\n",
    "            distances[0][t2] = t2\n",
    "\n",
    "        a = 0\n",
    "        b = 0\n",
    "        c = 0\n",
    "\n",
    "        for t1 in range(1, len(token1) + 1):\n",
    "            for t2 in range(1, len(token2) + 1):\n",
    "                if (token1[t1-1] == token2[t2-1]):\n",
    "                    distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "                else:\n",
    "                    a = distances[t1][t2 - 1]\n",
    "                    b = distances[t1 - 1][t2]\n",
    "                    c = distances[t1 - 1][t2 - 1]\n",
    "\n",
    "                    if (a <= b and a <= c):\n",
    "                        distances[t1][t2] = a + 1\n",
    "                    elif (b <= a and b <= c):\n",
    "                        distances[t1][t2] = b + 1\n",
    "                    else:\n",
    "                        distances[t1][t2] = c + 1\n",
    "\n",
    "\n",
    "        return 1 - ((distances[len(token1)][len(token2)])/max([len(token1), len(token2)]))\n",
    "    \n",
    "    def get_fuzzy_score_outlier(self, clean, dirty, corrected):\n",
    "        clean = re.findall(r'\\d+', clean)\n",
    "        dirty = re.findall(r'\\d+', dirty)\n",
    "        corrected = re.findall(r'\\d+', corrected)\n",
    "\n",
    "        if len(clean) != len(dirty) or len(dirty) != len(corrected) or len(corrected) != len(clean):\n",
    "            return 0\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        #for (o,d,c) in  zip(clean, dirty, corrected):\n",
    "        #    if (int(o) > int(d) and int(d) > int(c)) or (int(o) <int(d) and int(d) < int(c)):\n",
    "        #        return 0\n",
    "        #    if abs(int(o) - int(c)) < abs(int(o) - int(d)):\n",
    "        #        count += abs(int(o)-int(c)) / abs(int(o) - int(d))\n",
    "        \n",
    "        for (o,d,c) in  zip(clean, dirty, corrected):\n",
    "            if abs(int(o) - int(d)) != 0:\n",
    "                count += abs(int(o)-int(c)) / abs(int(o) - int(d))\n",
    "            if abs(int(o) - int(c)) < abs(int(o) - int(d)):\n",
    "                count += abs(int(o)-int(c)) / abs(int(o) - int(d))\n",
    "       \n",
    "        return count/len(clean)\n",
    "    \n",
    "    \n",
    "    def get_fuzzy_score_semantic_sentence(self, clean, dirty, corrected):\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "        embeddings1 = model.encode(clean, convert_to_tensor=True)\n",
    "        embeddings2 = model.encode(dirty, convert_to_tensor=True)\n",
    "        embeddings3 = model.encode(corrected, convert_to_tensor=True)\n",
    "\n",
    "        cosine_scores_clean_dirty = util.cos_sim(embeddings1, embeddings2)\n",
    "        cosine_scores_clean_corrected = util.cos_sim(embeddings1, embeddings3)\n",
    "\n",
    "\n",
    "        return (cosine_scores_clean_corrected[0][0].item() - 0.2) / (0.8) - (cosine_scores_clean_dirty[0][0].item() - 0.2) / (0.8)\n",
    "    \n",
    "    def get_semantic_score(self, s1, s2):\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "        embeddings1 = model.encode(s1, convert_to_tensor=True)\n",
    "        embeddings2 = model.encode(s2, convert_to_tensor=True)\n",
    "        score = util.cos_sim(embeddings1, embeddings2)\n",
    "        \n",
    "        return (score[0][0].item() - 0.2) / (0.8)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def get_data_cleaning_evaluation_fuzzy_semantic_sentences(self, num_metric=False):\n",
    "\n",
    "        \"\"\"\n",
    "        This method evaluates data cleaning process using fuzzy metrics\n",
    "        \"\"\"\n",
    "\n",
    "        pc_r = 0.0 #right partial corrections\n",
    "        pc_f = 0.0 #false partial corrections\n",
    "        ec_tp = 0.0\n",
    "        output_size = 0.0\n",
    "        for cell in self.error_corrected_val:\n",
    "            output_size += 1\n",
    "            if cell in self.error_clean_val:\n",
    "                if self.error_corrected_val[cell] == self.error_clean_val[cell]:\n",
    "                    ec_tp += 1.0\n",
    "                elif (num_metric and cell[1] in self.str_attr and cell[1] not in self.numer_attr) or (not num_metric and cell[1] in self.str_attr):\n",
    "                    metric_score = self.get_fuzzy_score_semantic_sentence(self.error_clean_val[cell], self.error_dirty_val[cell],  self.error_corrected_val[cell])\n",
    "                    ec_tp += metric_score\n",
    "                    if metric_score >= 0:\n",
    "                        pc_r += 1\n",
    "                    else:\n",
    "                        pc_f += 1\n",
    "\n",
    "        if num_metric:\n",
    "            ec_tp += self.numer_tp\n",
    "            ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "            ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "            ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "            return {\"Fuzzy Semantic Sentences Num Precision\": round(ec_p, 3),\"Fuzzy Semantic Sentences Num Recall\": round(ec_r, 3), \"Fuzzy Semantic Sentences Num F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "        else:    \n",
    "            ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "            ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "            ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "            return {\"Fuzzy Semantic Sentences Precision\": round(ec_p, 3),\"Fuzzy Semantic Sentences Recall\": round(ec_r, 3), \"Fuzzy Semantic Sentences F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "        \n",
    "        \n",
    "    def get_combined_score_evaluation(self, num_metric=False):\n",
    "        pc_r = 0.0 #right partial corrections\n",
    "        pc_f = 0.0 #false partial corrections\n",
    "        ec_tp = 0.0\n",
    "        output_size = 0.0\n",
    "        for cell in self.error_corrected_val:\n",
    "            output_size += 1\n",
    "            if cell in self.error_clean_val:\n",
    "                if self.error_corrected_val[cell] == self.error_clean_val[cell]:\n",
    "                    ec_tp += 1.0\n",
    "                elif (num_metric and cell[1] in self.str_attr and cell[1] not in self.numer_attr) or (not num_metric and cell[1] in self.str_attr):\n",
    "                    \n",
    "                    clean_dirty_semantic_score = self.get_semantic_score(self.error_clean_val[cell], self.error_dirty_val[cell])\n",
    "                    clean_dirty_string_score = self.get_string_score_avg(self.error_clean_val[cell], self.error_dirty_val[cell])\n",
    "                    clean_corrected_semantic_score = self.get_semantic_score(self.error_clean_val[cell], self.error_corrected_val[cell])\n",
    "                    clean_corrected_string_score = self.get_string_score_avg(self.error_clean_val[cell], self.error_corrected_val[cell])\n",
    "                    \n",
    "                    combined_score = self.get_combined_score(clean_dirty_semantic_score, clean_dirty_string_score, clean_corrected_semantic_score, clean_corrected_string_score)\n",
    "                    \n",
    "                    ec_tp += combined_score\n",
    "                    if combined_score >= 0:\n",
    "                        pc_r += 1\n",
    "                    else:\n",
    "                        pc_f += 1\n",
    "                        \n",
    "        ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "        ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "        ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "        return {\"Combined Precision\": round(ec_p, 3),\"Combined Recall\": round(ec_r, 3), \"Combined F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "                        \n",
    "                        \n",
    "    def get_string_score_avg(self, s1, s2):\n",
    "        score_avg = ((self.levenshteinDistanceChar(s1, s2) / max([len(s1), len(s2)])) + self.monge_elkan(s1, s2) + self.jaro_winkler_distance(s1, s2)) / 3\n",
    "        return score_avg\n",
    "    \n",
    "    def get_combined_score(self, cd_semantic, cd_string, cc_semantic, cc_string):\n",
    "        \n",
    "        threshold = 0.7\n",
    "        \n",
    "        string_score = cc_string - cd_string\n",
    "        semantic_score = cc_semantic - cd_semantic\n",
    "        avg_score = (string_score + semantic_score) / 2\n",
    "        \n",
    "        \n",
    "        #semantic score high and string score high\n",
    "        if cd_semantic >= threshold and cc_string >=threshold:\n",
    "            \n",
    "            if cc_semantic >= threshold and cc_string >=threshold:\n",
    "                return avg_score\n",
    "            \n",
    "            elif cc_semantic >= threshold and cc_string < threshold:\n",
    "                return string_score\n",
    "\n",
    "            elif cc_semantic < threshold and cc_string >= threshold:\n",
    "                return semantic_score\n",
    "\n",
    "            elif cc_semantic < threshold and cc_string < threshold:\n",
    "                return avg_score\n",
    "        \n",
    "        \n",
    "        #semantic score high and string score low\n",
    "        elif cd_semantic >= threshold and cd_string < threshold:\n",
    "            \n",
    "            if cc_semantic >= threshold and cc_string >=threshold:\n",
    "                return string_score\n",
    "            \n",
    "            elif cc_semantic >= threshold and cc_string < threshold:\n",
    "                return avg_score\n",
    "                \n",
    "            elif cc_semantic < threshold and cc_string >= threshold:\n",
    "                return avg_score \n",
    "\n",
    "            elif cc_semantic < threshold and cc_string < threshold:\n",
    "                return semantic_score\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #semantic score low and string score high\n",
    "        elif cd_semantic < threshold and cd_string >= threshold:\n",
    "            \n",
    "            if cc_semantic >= threshold and cc_string >=threshold:\n",
    "                return semantic_score\n",
    "            \n",
    "            elif cc_semantic >= threshold and cc_string < threshold:\n",
    "                return avg_score \n",
    "\n",
    "            elif cc_semantic < threshold and cc_string >= threshold:\n",
    "                return avg_score \n",
    "\n",
    "            elif cc_semantic < threshold and cc_string < threshold:\n",
    "                return string_score\n",
    "        \n",
    "        \n",
    "        #semantic score low and string score low\n",
    "        elif cd_semantic < threshold and cd_string < threshold:\n",
    "            \n",
    "            if cc_semantic >= threshold and cc_string >=threshold:\n",
    "                return avg_score\n",
    "            \n",
    "            elif cc_semantic >= threshold and cc_string < threshold:\n",
    "                return semantic_score\n",
    "\n",
    "            elif cc_semantic < threshold and cc_string >= threshold:\n",
    "                return string_score\n",
    "\n",
    "            elif cc_semantic < threshold and cc_string < threshold:\n",
    "                return avg_score\n",
    "            \n",
    "        return avg_score\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "133d027b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calhoun', 'calhoxn', 'madison', -0.2857142857142857]\n",
      "['calhoun', 'caxhoun', 'madison', -0.2857142857142857]\n",
      "['houston', 'housxon', 'madison', -0.2857142857142857]\n",
      "['coffee', 'coffxx', 'clarke', -0.2222222222222222]\n",
      "['elmore', 'elmoxe', 'clarke', -0.22222222222222232]\n",
      "['elmore', 'elxore', 'clarke', -0.22222222222222232]\n",
      "['fayette', 'fayexxe', 'madison', -0.3809523809523811]\n",
      "['etowah', 'etowxh', 'clarke', -0.44444444444444453]\n",
      "['butler', 'butxer', 'clarke', -0.3333333333333335]\n",
      "['coffee', 'cxffee', 'clarke', -0.3333333333333335]\n",
      "['scip-vte-1', 'sxip-vte-1', 'sxip-vtf-1', -0.06666666666666654]\n",
      "['scip-vte-1', 'scip-vtx-1', 'scip-vtf-1', 0.0]\n",
      "['calhoun', 'calhoxn', 'madison', 0.8571428571428572, 0.2857142857142857, -0.5714285714285715]\n",
      "['calhoun', 'caxhoun', 'madison', 0.8571428571428572, 0.2857142857142857, -0.5714285714285715]\n",
      "['houston', 'housxon', 'madison', 0.8571428571428572, 0.2857142857142857, -0.5714285714285715]\n",
      "['coffee', 'coffxx', 'clarke', 0.6666666666666667, 0.33333333333333337, -0.33333333333333337]\n",
      "['elmore', 'elmoxe', 'clarke', 0.8333333333333334, 0.33333333333333337, -0.5]\n",
      "['elmore', 'elxore', 'clarke', 0.8333333333333334, 0.33333333333333337, -0.5]\n",
      "['fayette', 'fayexxe', 'madison', 0.7142857142857143, 0.1428571428571429, -0.5714285714285714]\n",
      "['etowah', 'etowxh', 'clarke', 0.8333333333333334, 0.0, -0.8333333333333334]\n",
      "['butler', 'butxer', 'clarke', 0.8333333333333334, 0.0, -0.8333333333333334]\n",
      "['coffee', 'cxffee', 'clarke', 0.8333333333333334, 0.33333333333333337, -0.5]\n",
      "['scip-vte-1', 'sxip-vte-1', 'sxip-vtf-1', 0.9, 0.8, -0.09999999999999998]\n",
      "['scip-vte-1', 'scip-vtx-1', 'scip-vtf-1', 0.9, 0.9, 0.0]\n",
      "['calhoun', 'calhoxn', 'madison', -0.2857142857142857]\n",
      "['calhoun', 'caxhoun', 'madison', -0.2857142857142857]\n",
      "['houston', 'housxon', 'madison', -0.2857142857142857]\n",
      "['coffee', 'coffxx', 'clarke', -0.2222222222222222]\n",
      "['elmore', 'elmoxe', 'clarke', -0.22222222222222232]\n",
      "['elmore', 'elxore', 'clarke', -0.22222222222222232]\n",
      "['fayette', 'fayexxe', 'madison', -0.3809523809523811]\n",
      "['etowah', 'etowxh', 'clarke', -0.44444444444444453]\n",
      "['butler', 'butxer', 'clarke', -0.3333333333333335]\n",
      "['coffee', 'cxffee', 'clarke', -0.3333333333333335]\n",
      "['scip-vte-1', 'sxip-vte-1', 'sxip-vtf-1', -0.06666666666666654]\n",
      "['scip-vte-1', 'scip-vtx-1', 'scip-vtf-1', 0.0]\n",
      "['calhoun', 'calhoxn', 'madison', 0.8571428571428572, 0.2857142857142857, -0.5714285714285715]\n",
      "['calhoun', 'caxhoun', 'madison', 0.8571428571428572, 0.2857142857142857, -0.5714285714285715]\n",
      "['houston', 'housxon', 'madison', 0.8571428571428572, 0.2857142857142857, -0.5714285714285715]\n",
      "['coffee', 'coffxx', 'clarke', 0.6666666666666667, 0.33333333333333337, -0.33333333333333337]\n",
      "['elmore', 'elmoxe', 'clarke', 0.8333333333333334, 0.33333333333333337, -0.5]\n",
      "['elmore', 'elxore', 'clarke', 0.8333333333333334, 0.33333333333333337, -0.5]\n",
      "['fayette', 'fayexxe', 'madison', 0.7142857142857143, 0.1428571428571429, -0.5714285714285714]\n",
      "['etowah', 'etowxh', 'clarke', 0.8333333333333334, 0.0, -0.8333333333333334]\n",
      "['butler', 'butxer', 'clarke', 0.8333333333333334, 0.0, -0.8333333333333334]\n",
      "['coffee', 'cxffee', 'clarke', 0.8333333333333334, 0.33333333333333337, -0.5]\n",
      "['scip-vte-1', 'sxip-vte-1', 'sxip-vtf-1', 0.9, 0.8, -0.09999999999999998]\n",
      "['scip-vte-1', 'scip-vtx-1', 'scip-vtf-1', 0.9, 0.9, 0.0]\n",
      "{'Combined Precision': 0.852, 'Combined Recall': 0.496, 'Combined F1': 0.627, 'PC R': 5.0, 'PC F': 7.0}\n",
      "\n",
      "{'Precision': 0.858, 'Recall': 0.499, 'F1': 0.631, 'Amount of fixed data errors': 296.0}\n",
      "{'Fuzzy JW Precision': 0.848, 'Fuzzy JW Recall': 0.493, 'Fuzzy JW F1': 0.623, 'PC R': 1.0, 'PC F': 11.0}\n",
      "{'Fuzzy ME Precision': 0.861, 'Fuzzy ME Recall': 0.501, 'Fuzzy ME F1': 0.633, 'PC R': 11.0, 'PC F': 1.0}\n",
      "{'Fuzzy LD Char Precision': 0.838, 'Fuzzy LD Char Recall': 0.487, 'Fuzzy LD Char F1': 0.616, 'PC R': 1.0, 'PC F': 11.0}\n",
      "{'Fuzzy Semantic Sentences Precision': 0.851, 'Fuzzy Semantic Sentences Recall': 0.495, 'Fuzzy Semantic Sentences F1': 0.626, 'PC R': 5.0, 'PC F': 7.0}\n",
      "\n",
      "{'Fuzzy JW Num Precision': 0.866, 'Fuzzy JW Num Recall': 0.504, 'Fuzzy JW Num F1': 0.637, 'PC R': 1.0, 'PC F': 11.0}\n",
      "{'Fuzzy ME Num Precision': 0.879, 'Fuzzy ME Num Recall': 0.511, 'Fuzzy ME Num F1': 0.647, 'PC R': 11.0, 'PC F': 1.0}\n",
      "{'Fuzzy LD Char Num Precision': 0.857, 'Fuzzy LD Char Num Recall': 0.498, 'Fuzzy LD Char Num F1': 0.63, 'PC R': 1.0, 'PC F': 11.0}\n",
      "{'Fuzzy Semantic Sentences Num Precision': 0.87, 'Fuzzy Semantic Sentences Num Recall': 0.506, 'Fuzzy Semantic Sentences Num F1': 0.64, 'PC R': 5.0, 'PC F': 7.0}\n"
     ]
    }
   ],
   "source": [
    "metric_dict_hospital = {\n",
    "    \"clean_data_path\": \"../datasets/hospital/clean.csv\",\n",
    "    \"dirty_data_path\": \"../datasets/hospital/dirty.csv\",\n",
    "    \"corrected_data_path\": \"../datasets/hospital/baran_repaired1.csv\",\n",
    "    \"str_attr\": [2,3,6,7,9, 11, 12, 13, 14, 15, 16, 19],\n",
    "    \"short_str_attr\": [3, 6, 7, 9, 13, 17, 18, 19],\n",
    "    \"long_str_attr\": [2,11, 12, 14, 15, 16],\n",
    "    \"numer_attr\": [17, 18]\n",
    "    \n",
    "}\n",
    "\n",
    "m_hospital1 = Metrics(metric_dict_hospital)\n",
    "\n",
    "m_hospital1.print_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "224090e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no', 'xo', 'yes', -0.6666666666666666]\n",
      "['no', 'xo', 'yes', -0.6666666666666666]\n",
      "['no', 'xo', 'yes', 0.5, 0.0, -0.5]\n",
      "['no', 'xo', 'yes', 0.5, 0.0, -0.5]\n",
      "['no', 'xo', 'yes', -0.6666666666666666]\n",
      "['no', 'xo', 'yes', -0.6666666666666666]\n",
      "['no', 'xo', 'yes', 0.5, 0.0, -0.5]\n",
      "['no', 'xo', 'yes', 0.5, 0.0, -0.5]\n",
      "{'Combined Precision': 0.871, 'Combined Recall': 0.502, 'Combined F1': 0.637, 'PC R': 2.0, 'PC F': 0.0}\n",
      "\n",
      "{'Precision': 0.87, 'Recall': 0.501, 'F1': 0.636, 'Amount of fixed data errors': 293.0}\n",
      "{'Fuzzy JW Precision': 0.866, 'Fuzzy JW Recall': 0.498, 'Fuzzy JW F1': 0.633, 'PC R': 0.0, 'PC F': 2.0}\n",
      "{'Fuzzy ME Precision': 0.87, 'Fuzzy ME Recall': 0.501, 'Fuzzy ME F1': 0.636, 'PC R': 2.0, 'PC F': 0.0}\n",
      "{'Fuzzy LD Char Precision': 0.867, 'Fuzzy LD Char Recall': 0.499, 'Fuzzy LD Char F1': 0.633, 'PC R': 0.0, 'PC F': 2.0}\n",
      "{'Fuzzy Semantic Sentences Precision': 0.874, 'Fuzzy Semantic Sentences Recall': 0.503, 'Fuzzy Semantic Sentences F1': 0.638, 'PC R': 2.0, 'PC F': 0.0}\n",
      "\n",
      "{'Fuzzy JW Num Precision': 0.892, 'Fuzzy JW Num Recall': 0.514, 'Fuzzy JW Num F1': 0.652, 'PC R': 0.0, 'PC F': 2.0}\n",
      "{'Fuzzy ME Num Precision': 0.897, 'Fuzzy ME Num Recall': 0.516, 'Fuzzy ME Num F1': 0.655, 'PC R': 2.0, 'PC F': 0.0}\n",
      "{'Fuzzy LD Char Num Precision': 0.893, 'Fuzzy LD Char Num Recall': 0.514, 'Fuzzy LD Char Num F1': 0.653, 'PC R': 0.0, 'PC F': 2.0}\n",
      "{'Fuzzy Semantic Sentences Num Precision': 0.9, 'Fuzzy Semantic Sentences Num Recall': 0.518, 'Fuzzy Semantic Sentences Num F1': 0.658, 'PC R': 2.0, 'PC F': 0.0}\n"
     ]
    }
   ],
   "source": [
    "metric_dict_hospital = {\n",
    "    \"clean_data_path\": \"../datasets/hospital/clean.csv\",\n",
    "    \"dirty_data_path\": \"../datasets/hospital/dirty.csv\",\n",
    "    \"corrected_data_path\": \"../datasets/hospital/baran_repaired2.csv\",\n",
    "    \"str_attr\": [2,3,6,7,9, 11, 12, 13, 14, 15, 16, 19],\n",
    "    \"short_str_attr\": [3, 6, 7, 9, 13, 17, 18, 19],\n",
    "    \"long_str_attr\": [2,11, 12, 14, 15, 16],\n",
    "    \"numer_attr\": [17, 18]\n",
    "    \n",
    "}\n",
    "\n",
    "m_hospital2 = Metrics(metric_dict_hospital)\n",
    "\n",
    "m_hospital2.print_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f997c3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wedowee', 'wxdowxx', 'birmingham', -0.7142857142857143]\n",
      "['wedowee', 'wxdowxx', 'birmingham', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['wedowee', 'wxdowxx', 'birmingham', -0.7142857142857143]\n",
      "['wedowee', 'wxdowxx', 'birmingham', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "{'Combined Precision': 0.932, 'Combined Recall': 0.456, 'Combined F1': 0.612, 'PC R': 1.0, 'PC F': 0.0}\n",
      "\n",
      "{'Precision': 0.932, 'Recall': 0.456, 'F1': 0.612, 'Amount of fixed data errors': 249.0}\n",
      "{'Fuzzy JW Precision': 0.929, 'Fuzzy JW Recall': 0.454, 'Fuzzy JW F1': 0.61, 'PC R': 0.0, 'PC F': 1.0}\n",
      "{'Fuzzy ME Precision': 0.932, 'Fuzzy ME Recall': 0.456, 'Fuzzy ME F1': 0.612, 'PC R': 1.0, 'PC F': 0.0}\n",
      "{'Fuzzy LD Char Precision': 0.929, 'Fuzzy LD Char Recall': 0.455, 'Fuzzy LD Char F1': 0.611, 'PC R': 0.0, 'PC F': 1.0}\n",
      "{'Fuzzy Semantic Sentences Precision': 0.933, 'Fuzzy Semantic Sentences Recall': 0.456, 'Fuzzy Semantic Sentences F1': 0.613, 'PC R': 1.0, 'PC F': 0.0}\n",
      "\n",
      "{'Fuzzy JW Num Precision': 0.935, 'Fuzzy JW Num Recall': 0.457, 'Fuzzy JW Num F1': 0.614, 'PC R': 0.0, 'PC F': 1.0}\n",
      "{'Fuzzy ME Num Precision': 0.938, 'Fuzzy ME Num Recall': 0.459, 'Fuzzy ME Num F1': 0.616, 'PC R': 1.0, 'PC F': 0.0}\n",
      "{'Fuzzy LD Char Num Precision': 0.936, 'Fuzzy LD Char Num Recall': 0.458, 'Fuzzy LD Char Num F1': 0.615, 'PC R': 0.0, 'PC F': 1.0}\n",
      "{'Fuzzy Semantic Sentences Num Precision': 0.939, 'Fuzzy Semantic Sentences Num Recall': 0.459, 'Fuzzy Semantic Sentences Num F1': 0.617, 'PC R': 1.0, 'PC F': 0.0}\n"
     ]
    }
   ],
   "source": [
    "metric_dict_hospital = {\n",
    "    \"clean_data_path\": \"../datasets/hospital/clean.csv\",\n",
    "    \"dirty_data_path\": \"../datasets/hospital/dirty.csv\",\n",
    "    \"corrected_data_path\": \"../datasets/hospital/baran_repaired3.csv\",\n",
    "    \"str_attr\": [2,3,6,7,9, 11, 12, 13, 14, 15, 16, 19],\n",
    "    \"short_str_attr\": [3, 6, 7, 9, 13, 17, 18, 19],\n",
    "    \"long_str_attr\": [2,11, 12, 14, 15, 16],\n",
    "    \"numer_attr\": [17, 18]\n",
    "    \n",
    "}\n",
    "\n",
    "m_hospital3 = Metrics(metric_dict_hospital)\n",
    "\n",
    "m_hospital3.print_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa9299fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2505 u s highway 431 north', '2505xuxsxhighwayx431xnorth', '8000 alabama highway 69', -0.26831863788385535]\n",
      "['702 n main st', '702xnxmainxst', '849 south three notch street', -0.39865689865689863]\n",
      "['702 n main st', '702 x maix st', '849 south three notch street', -0.44993894993894995]\n",
      "['4370 west main street', '4370xwestxmainxstreet', '1108 ross clark circle', -0.3611832611832613]\n",
      "['1530 u s highway 43', '1530xuxsxhighwayx43', '1256 military street south', -0.36414754835807467]\n",
      "['1256 military street south', '1256 military street sxuth', '1530 u s highway 43', -0.47885739991003157]\n",
      "['2505 u s highway 431 north', '2505xuxsxhighwayx431xnorth', '8000 alabama highway 69', 0.8076923076923077, 0.2692307692307693, -0.5384615384615384]\n",
      "['702 n main st', '702xnxmainxst', '849 south three notch street', 0.7692307692307692, 0.2142857142857143, -0.5549450549450549]\n",
      "['702 n main st', '702 x maix st', '849 south three notch street', 0.8461538461538461, 0.2142857142857143, -0.6318681318681318]\n",
      "['4370 west main street', '4370xwestxmainxstreet', '1108 ross clark circle', 0.8571428571428572, 0.2727272727272727, -0.5844155844155845]\n",
      "['1530 u s highway 43', '1530xuxsxhighwayx43', '1256 military street south', 0.7894736842105263, 0.15384615384615385, -0.6356275303643725]\n",
      "['1256 military street south', '1256 military street sxuth', '1530 u s highway 43', 0.9615384615384616, 0.15384615384615385, -0.8076923076923077]\n",
      "['2505 u s highway 431 north', '2505xuxsxhighwayx431xnorth', '8000 alabama highway 69', -0.26831863788385535]\n",
      "['702 n main st', '702xnxmainxst', '849 south three notch street', -0.39865689865689863]\n",
      "['702 n main st', '702 x maix st', '849 south three notch street', -0.44993894993894995]\n",
      "['4370 west main street', '4370xwestxmainxstreet', '1108 ross clark circle', -0.3611832611832613]\n",
      "['1530 u s highway 43', '1530xuxsxhighwayx43', '1256 military street south', -0.36414754835807467]\n",
      "['1256 military street south', '1256 military street sxuth', '1530 u s highway 43', -0.47885739991003157]\n",
      "['2505 u s highway 431 north', '2505xuxsxhighwayx431xnorth', '8000 alabama highway 69', 0.8076923076923077, 0.2692307692307693, -0.5384615384615384]\n",
      "['702 n main st', '702xnxmainxst', '849 south three notch street', 0.7692307692307692, 0.2142857142857143, -0.5549450549450549]\n",
      "['702 n main st', '702 x maix st', '849 south three notch street', 0.8461538461538461, 0.2142857142857143, -0.6318681318681318]\n",
      "['4370 west main street', '4370xwestxmainxstreet', '1108 ross clark circle', 0.8571428571428572, 0.2727272727272727, -0.5844155844155845]\n",
      "['1530 u s highway 43', '1530xuxsxhighwayx43', '1256 military street south', 0.7894736842105263, 0.15384615384615385, -0.6356275303643725]\n",
      "['1256 military street south', '1256 military street sxuth', '1530 u s highway 43', 0.9615384615384616, 0.15384615384615385, -0.8076923076923077]\n",
      "{'Combined Precision': 0.872, 'Combined Recall': 0.442, 'Combined F1': 0.587, 'PC R': 1.0, 'PC F': 5.0}\n",
      "\n",
      "{'Precision': 0.876, 'Recall': 0.444, 'F1': 0.589, 'Amount of fixed data errors': 258.0}\n",
      "{'Fuzzy JW Precision': 0.867, 'Fuzzy JW Recall': 0.439, 'Fuzzy JW F1': 0.583, 'PC R': 0.0, 'PC F': 6.0}\n",
      "{'Fuzzy ME Precision': 0.878, 'Fuzzy ME Recall': 0.445, 'Fuzzy ME F1': 0.591, 'PC R': 6.0, 'PC F': 0.0}\n",
      "{'Fuzzy LD Char Precision': 0.861, 'Fuzzy LD Char Recall': 0.437, 'Fuzzy LD Char F1': 0.58, 'PC R': 0.0, 'PC F': 6.0}\n",
      "{'Fuzzy Semantic Sentences Precision': 0.872, 'Fuzzy Semantic Sentences Recall': 0.442, 'Fuzzy Semantic Sentences F1': 0.586, 'PC R': 1.0, 'PC F': 5.0}\n",
      "\n",
      "{'Fuzzy JW Num Precision': 0.873, 'Fuzzy JW Num Recall': 0.443, 'Fuzzy JW Num F1': 0.587, 'PC R': 0.0, 'PC F': 6.0}\n",
      "{'Fuzzy ME Num Precision': 0.884, 'Fuzzy ME Num Recall': 0.448, 'Fuzzy ME Num F1': 0.595, 'PC R': 6.0, 'PC F': 0.0}\n",
      "{'Fuzzy LD Char Num Precision': 0.868, 'Fuzzy LD Char Num Recall': 0.44, 'Fuzzy LD Char Num F1': 0.584, 'PC R': 0.0, 'PC F': 6.0}\n",
      "{'Fuzzy Semantic Sentences Num Precision': 0.878, 'Fuzzy Semantic Sentences Num Recall': 0.445, 'Fuzzy Semantic Sentences Num F1': 0.591, 'PC R': 1.0, 'PC F': 5.0}\n"
     ]
    }
   ],
   "source": [
    "metric_dict_hospital = {\n",
    "    \"clean_data_path\": \"../datasets/hospital/clean.csv\",\n",
    "    \"dirty_data_path\": \"../datasets/hospital/dirty.csv\",\n",
    "    \"corrected_data_path\": \"../datasets/hospital/baran_repaired4.csv\",\n",
    "    \"str_attr\": [2,3,6,7,9, 11, 12, 13, 14, 15, 16, 19],\n",
    "    \"short_str_attr\": [3, 6, 7, 9, 13, 17, 18, 19],\n",
    "    \"long_str_attr\": [2,11, 12, 14, 15, 16],\n",
    "    \"numer_attr\": [17, 18]\n",
    "    \n",
    "}\n",
    "\n",
    "m_hospital4 = Metrics(metric_dict_hospital)\n",
    "\n",
    "m_hospital4.print_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a48c99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Combined Precision': 0.914, 'Combined Recall': 0.462, 'Combined F1': 0.614, 'PC R': 0.0, 'PC F': 0.0}\n",
      "\n",
      "{'Precision': 0.914, 'Recall': 0.462, 'F1': 0.614, 'Amount of fixed data errors': 257.0}\n",
      "{'Fuzzy JW Precision': 0.914, 'Fuzzy JW Recall': 0.462, 'Fuzzy JW F1': 0.614, 'PC R': 0.0, 'PC F': 0.0}\n",
      "{'Fuzzy ME Precision': 0.914, 'Fuzzy ME Recall': 0.462, 'Fuzzy ME F1': 0.614, 'PC R': 0.0, 'PC F': 0.0}\n",
      "{'Fuzzy LD Char Precision': 0.914, 'Fuzzy LD Char Recall': 0.462, 'Fuzzy LD Char F1': 0.614, 'PC R': 0.0, 'PC F': 0.0}\n",
      "{'Fuzzy Semantic Sentences Precision': 0.914, 'Fuzzy Semantic Sentences Recall': 0.462, 'Fuzzy Semantic Sentences F1': 0.614, 'PC R': 0.0, 'PC F': 0.0}\n",
      "\n",
      "{'Fuzzy JW Num Precision': 0.938, 'Fuzzy JW Num Recall': 0.473, 'Fuzzy JW Num F1': 0.629, 'PC R': 0.0, 'PC F': 0.0}\n",
      "{'Fuzzy ME Num Precision': 0.938, 'Fuzzy ME Num Recall': 0.473, 'Fuzzy ME Num F1': 0.629, 'PC R': 0.0, 'PC F': 0.0}\n",
      "{'Fuzzy LD Char Num Precision': 0.938, 'Fuzzy LD Char Num Recall': 0.473, 'Fuzzy LD Char Num F1': 0.629, 'PC R': 0.0, 'PC F': 0.0}\n",
      "{'Fuzzy Semantic Sentences Num Precision': 0.938, 'Fuzzy Semantic Sentences Num Recall': 0.473, 'Fuzzy Semantic Sentences Num F1': 0.629, 'PC R': 0.0, 'PC F': 0.0}\n"
     ]
    }
   ],
   "source": [
    "metric_dict_hospital = {\n",
    "    \"clean_data_path\": \"../datasets/hospital/clean.csv\",\n",
    "    \"dirty_data_path\": \"../datasets/hospital/dirty.csv\",\n",
    "    \"corrected_data_path\": \"../datasets/hospital/baran_repaired5.csv\",\n",
    "    \"str_attr\": [2,3,6,7,9, 11, 12, 13, 14, 15, 16, 19],\n",
    "    \"short_str_attr\": [3, 6, 7, 9, 13, 17, 18, 19],\n",
    "    \"long_str_attr\": [2,11, 12, 14, 15, 16],\n",
    "    \"numer_attr\": [17, 18]\n",
    "    \n",
    "}\n",
    "\n",
    "m_hospital5 = Metrics(metric_dict_hospital)\n",
    "\n",
    "m_hospital5.print_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5723e3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of 5 runs\n",
      "{'Precision': 0.89, 'Recall': 0.472, 'F1': 0.616, 'Amount of fixed data errors': 296.0}\n",
      "{'Fuzzy JW Precision': 0.885, 'Fuzzy JW Recall': 0.469, 'Fuzzy JW F1': 0.613, 'PC R': 1.0, 'PC F': 11.0}\n",
      "{'Fuzzy ME Precision': 0.891, 'Fuzzy ME Recall': 0.473, 'Fuzzy ME F1': 0.617, 'PC R': 11.0, 'PC F': 1.0}\n",
      "{'Fuzzy LD Char Precision': 0.882, 'Fuzzy LD Char Recall': 0.468, 'Fuzzy LD Char F1': 0.611, 'PC R': 1.0, 'PC F': 11.0}\n",
      "{'Fuzzy Semantic Sentences Precision': 0.889, 'Fuzzy Semantic Sentences Recall': 0.472, 'Fuzzy Semantic Sentences F1': 0.615, 'PC R': 5.0, 'PC F': 7.0}\n",
      "\n",
      "{'Fuzzy JW Num Precision': 0.901, 'Fuzzy JW Num Recall': 0.478, 'Fuzzy JW Num F1': 0.624, 'PC R': 1.0, 'PC F': 11.0}\n",
      "{'Fuzzy ME Num Precision': 0.907, 'Fuzzy ME Num Recall': 0.481, 'Fuzzy ME Num F1': 0.628, 'PC R': 11.0, 'PC F': 1.0}\n",
      "{'Fuzzy LD Char Num Precision': 0.898, 'Fuzzy LD Char Num Recall': 0.477, 'Fuzzy LD Char Num F1': 0.622, 'PC R': 1.0, 'PC F': 11.0}\n",
      "{'Fuzzy Semantic Sentences Num Precision': 0.905, 'Fuzzy Semantic Sentences Num Recall': 0.48, 'Fuzzy Semantic Sentences Num F1': 0.627, 'PC R': 5.0, 'PC F': 7.0}\n"
     ]
    }
   ],
   "source": [
    "def calc_avg(m_dict_list):\n",
    "    \n",
    "    result = m_dict_list[0].copy()\n",
    "    \n",
    "    \n",
    "    for i in range(0,3):\n",
    "        if len(m_dict_list[i]) > 1:\n",
    "            v = 0\n",
    "            for j in range(0, len(m_dict_list)):\n",
    "                v += list(m_dict_list[j].values())[i]\n",
    "            result[list(m_dict_list[0].keys())[i]] = round(v/len(m_dict_list),3)\n",
    "\n",
    "    print(result)\n",
    "    #return result\n",
    "    \n",
    "print(\"Average of 5 runs\")\n",
    "calc_avg([m_hospital1.standard_metric, m_hospital2.standard_metric, m_hospital3.standard_metric, m_hospital4.standard_metric, m_hospital5.standard_metric])\n",
    "#calc_avg([m_hospital1.fuzzy_metric, m_hospital2.fuzzy_metric, m_hospital3.fuzzy_metric, m_hospital4.fuzzy_metric, m_hospital5.fuzzy_metric])\n",
    "#calc_avg([m_hospital1.fuzzy_alt_metric, m_hospital2.fuzzy_alt_metric, m_hospital3.fuzzy_alt_metric, m_hospital4.fuzzy_alt_metric, m_hospital5.fuzzy_alt_metric])\n",
    "calc_avg([m_hospital1.fuzzy_jw, m_hospital2.fuzzy_jw, m_hospital3.fuzzy_jw, m_hospital4.fuzzy_jw, m_hospital5.fuzzy_jw])\n",
    "calc_avg([m_hospital1.fuzzy_me, m_hospital2.fuzzy_me, m_hospital3.fuzzy_me, m_hospital4.fuzzy_me, m_hospital5.fuzzy_me])\n",
    "#calc_avg([m_hospital1.fuzzy_ld, m_hospital2.fuzzy_ld, m_hospital3.fuzzy_ld, m_hospital4.fuzzy_ld, m_hospital5.fuzzy_ld])\n",
    "#calc_avg([m_hospital1.fuzzy_ld_words, m_hospital2.fuzzy_ld_words, m_hospital3.fuzzy_ld_words, m_hospital4.fuzzy_ld_words, m_hospital5.fuzzy_ld_words])\n",
    "calc_avg([m_hospital1.fuzzy_ld_char, m_hospital2.fuzzy_ld_char, m_hospital3.fuzzy_ld_char, m_hospital4.fuzzy_ld_char, m_hospital5.fuzzy_ld_char])\n",
    "calc_avg([m_hospital1.fuzzy_semantics_sentences, m_hospital2.fuzzy_semantics_sentences, m_hospital3.fuzzy_semantics_sentences, m_hospital4.fuzzy_semantics_sentences, m_hospital5.fuzzy_semantics_sentences])\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "#calc_avg([m_hospital1.fuzzy_num_metric, m_hospital2.fuzzy_num_metric, m_hospital3.fuzzy_num_metric, m_hospital4.fuzzy_num_metric, m_hospital5.fuzzy_num_metric])\n",
    "#calc_avg([m_hospital1.fuzzy_alt_num_metric, m_hospital2.fuzzy_alt_num_metric, m_hospital3.fuzzy_alt_num_metric, m_hospital4.fuzzy_alt_num_metric, m_hospital5.fuzzy_alt_num_metric])\n",
    "calc_avg([m_hospital1.fuzzy_jw_num, m_hospital2.fuzzy_jw_num, m_hospital3.fuzzy_jw_num, m_hospital4.fuzzy_jw_num, m_hospital5.fuzzy_jw_num])\n",
    "calc_avg([m_hospital1.fuzzy_me_num, m_hospital2.fuzzy_me_num, m_hospital3.fuzzy_me_num, m_hospital4.fuzzy_me_num, m_hospital5.fuzzy_me_num])\n",
    "#calc_avg([m_hospital1.fuzzy_ld_num, m_hospital2.fuzzy_ld_num, m_hospital3.fuzzy_ld_num, m_hospital4.fuzzy_ld_num, m_hospital5.fuzzy_ld_num])\n",
    "#calc_avg([m_hospital1.fuzzy_ld_words_num, m_hospital2.fuzzy_ld_words_num, m_hospital3.fuzzy_ld_words_num, m_hospital4.fuzzy_ld_words_num, m_hospital5.fuzzy_ld_words_num])\n",
    "calc_avg([m_hospital1.fuzzy_ld_char_num, m_hospital2.fuzzy_ld_char_num, m_hospital3.fuzzy_ld_char_num, m_hospital4.fuzzy_ld_char_num, m_hospital5.fuzzy_ld_char_num])\n",
    "calc_avg([m_hospital1.fuzzy_semantics_sentences_num, m_hospital2.fuzzy_semantics_sentences_num, m_hospital3.fuzzy_semantics_sentences_num, m_hospital4.fuzzy_semantics_sentences_num, m_hospital5.fuzzy_semantics_sentences_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78ce113b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Combined Precision': 0.888, 'Combined Recall': 0.472, 'Combined F1': 0.615, 'PC R': 5.0, 'PC F': 7.0}\n"
     ]
    }
   ],
   "source": [
    "calc_avg([m_hospital1.combined_metric, m_hospital2.combined_metric, m_hospital3.combined_metric, m_hospital4.combined_metric, m_hospital5.combined_metric])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
