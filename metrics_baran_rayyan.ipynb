{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f895b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import html\n",
    "import re\n",
    "import numpy\n",
    "import sys\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from math import floor, ceil\n",
    "#from gensim.models import KeyedVectors\n",
    "#from gensim.downloader import load\n",
    "\n",
    "#model = load('word2vec-google-news-300')\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self, metric_dict):\n",
    "        #save paths to clean, dirty and corrected dataset\n",
    "        self.clean_path = metric_dict[\"clean_data_path\"]\n",
    "        self.dirty_path = metric_dict[\"dirty_data_path\"]\n",
    "        #self.corrected_path = metric_dict[\"corrected_data_path\"]\n",
    "        \n",
    "        #read csv-files of clean, dirty and corrected dataset\n",
    "        self.clean_data = self.read_csv_dataset(metric_dict[\"clean_data_path\"])\n",
    "        self.dirty_data = self.read_csv_dataset(metric_dict[\"dirty_data_path\"])\n",
    "        self.corrected_data = self.read_csv_dataset(metric_dict[\"corrected_data_path\"])\n",
    "        \n",
    "        #create dictionaries for errorneous cells\n",
    "        #save clean, dirty and corrected values of erroneous cells seperately\n",
    "        self.error_clean_val = self.get_dataframes_difference(self.dirty_data, self.clean_data) #clean values\n",
    "        self.error_dirty_val = self.get_dataframes_difference(self.clean_data, self.dirty_data) #dirty values\n",
    "        self.error_corrected_val = self.get_error_corrected_val() #corrected values\n",
    "        \n",
    "        #save attribute classification\n",
    "        self.str_attr = metric_dict[\"str_attr\"]\n",
    "        self.short_str_attr = metric_dict[\"short_str_attr\"]\n",
    "        self.long_str_attr = metric_dict[\"long_str_attr\"]\n",
    "        self.numer_attr = metric_dict[\"numer_attr\"]\n",
    "        \n",
    "        #calculate \"normal\" metrics precision, recall, f1\n",
    "        self.standard_metric = self.get_data_cleaning_evaluation()\n",
    "        \n",
    "        #numeric TP\n",
    "        self.numer_tp = self.get_numer_tp()\n",
    "        \n",
    "        #calculate and save fuzzy metrics\n",
    "        #self.fuzzy_alt_metric = self.get_data_cleaning_evaluation_fuzzy_alt()\n",
    "        self.fuzzy_jw = self.get_data_cleaning_evaluation_fuzzy_JW()\n",
    "        self.fuzzy_me = self.get_data_cleaning_evaluation_fuzzy_ME()\n",
    "        #self.fuzzy_ld_words = self.get_data_cleaning_evaluation_fuzzy_LD_Words() \n",
    "        self.fuzzy_ld_char = self.get_data_cleaning_evaluation_fuzzy_LD_Char() \n",
    "        #self.fuzzy_ld = self.get_data_cleaning_evaluation_fuzzy_LD() if self.short_str_attr or self.long_str_attr else {\"LD Message\": \"short or long string attributes not declared\"}\n",
    "        self.fuzzy_semantics_sentences = self.get_data_cleaning_evaluation_fuzzy_semantic_sentences()\n",
    "        \n",
    "        #fuzzy metrics combined with outlier metric\n",
    "        if self.numer_attr:\n",
    "            #self.fuzzy_alt_num_metric = self.get_data_cleaning_evaluation_fuzzy_alt(True)\n",
    "            self.fuzzy_jw_num = self.get_data_cleaning_evaluation_fuzzy_JW(True)\n",
    "            self.fuzzy_me_num = self.get_data_cleaning_evaluation_fuzzy_ME(True)\n",
    "            #self.fuzzy_ld_words_num = self.get_data_cleaning_evaluation_fuzzy_LD_Words(True)\n",
    "            self.fuzzy_ld_char_num = self.get_data_cleaning_evaluation_fuzzy_LD_Char(True)\n",
    "            #self.fuzzy_ld_num = self.get_data_cleaning_evaluation_fuzzy_LD(True) if self.short_str_attr or self.long_str_attr else {\"LD Num Message\": \"short or long string attributes not declared\"}\n",
    "            self.fuzzy_semantics_sentences_num = self.get_data_cleaning_evaluation_fuzzy_semantic_sentences(True)\n",
    "            \n",
    "        #average metrics\n",
    "        self.avg_string_metric = self.get_string_metric_avg()\n",
    "        self.avg_string_semantic_metric = self.get_string_semantic_metric_avg()\n",
    "        \n",
    "        #combined evaluation\n",
    "        self.combined_metric = self.get_combined_score_evaluation()\n",
    "\n",
    "        \n",
    "    def print_metrics(self):\n",
    "        print(self.combined_metric)\n",
    "        print(\"\")\n",
    "        print(self.standard_metric)\n",
    "        #print(self.fuzzy_alt_metric)\n",
    "        print(self.fuzzy_jw)\n",
    "        print(self.fuzzy_me)\n",
    "        #print(self.fuzzy_ld_words)\n",
    "        print(self.fuzzy_ld_char)\n",
    "        #print(self.fuzzy_ld)\n",
    "        #print(self.fuzzy_semantics_words)\n",
    "        print(self.fuzzy_semantics_sentences)\n",
    "        print(\"\")\n",
    "        \n",
    "        if self.numer_attr:\n",
    "            #print(self.fuzzy_alt_num_metric)\n",
    "            print(self.fuzzy_jw_num)\n",
    "            print(self.fuzzy_me_num)\n",
    "            #print(self.fuzzy_ld_words_num)\n",
    "            print(self.fuzzy_ld_char_num)\n",
    "            #print(self.fuzzy_ld_num)\n",
    "            #print(self.fuzzy_semantics_words_num)\n",
    "            print(self.fuzzy_semantics_sentences_num)\n",
    "            \n",
    "        else:\n",
    "            print({\"Num Metrics\": \"numeric attributes not declared\"})\n",
    "\n",
    "        \n",
    "    def read_csv_dataset(self, dataset_path):\n",
    "        \"\"\"\n",
    "        This method reads a dataset from a csv file path.\n",
    "        \"\"\"\n",
    "        dataframe = pandas.read_csv(dataset_path, sep=\",\", header=\"infer\", encoding=\"utf-8\", dtype=str,\n",
    "                                    keep_default_na=False, low_memory=False).applymap(self.value_normalizer)\n",
    "        return dataframe\n",
    "    \n",
    "    @staticmethod\n",
    "    def value_normalizer(value):\n",
    "        \"\"\"\n",
    "        This method takes a value and minimally normalizes it.\n",
    "        \"\"\"\n",
    "        value = html.unescape(value)\n",
    "        value = re.sub(\"[\\t\\n ]+\", \" \", value, re.UNICODE)\n",
    "        value = value.strip(\"\\t\\n \")\n",
    "        return value\n",
    "    \n",
    "    def get_string_metric_avg(self):\n",
    "        if True:\n",
    "            ld_words_char_avg_p = list(self.fuzzy_ld_char.values())[0]\n",
    "            ld_words_char_avg_r = list(self.fuzzy_ld_char.values())[1]\n",
    "            ld_words_char_avg_f1 = list(self.fuzzy_ld_char.values())[2]\n",
    "            avg_precision = (list(self.fuzzy_jw.values()))[0] + (list(self.fuzzy_me.values()))[0] +  ld_words_char_avg_p / 3\n",
    "            avg_recall = (list(self.fuzzy_jw.values()))[1] + (list(self.fuzzy_me.values()))[1] +  ld_words_char_avg_r / 3\n",
    "            avg_f1 = (list(self.fuzzy_jw.values()))[2] + (list(self.fuzzy_me.values()))[2] +  ld_words_char_avg_f1 / 3\n",
    "        \n",
    "        else:\n",
    "            avg_precision = (list(self.fuzzy_jw.values())[0] + list(self.fuzzy_me.values())[0] + list(self.fuzzy_ld.values())[0])/3\n",
    "            avg_recall = (list(self.fuzzy_jw.values())[1] + list(self.fuzzy_me.values())[1] + list(self.fuzzy_ld.values())[1])/3\n",
    "            avg_f1 = (list(self.fuzzy_jw.values())[2] + list(self.fuzzy_me.values())[2] + list(self.fuzzy_ld.values())[0])/3\n",
    "\n",
    "        return {\"Average of String Metrics Precision\": round(avg_precision,3), \"Average of String Metrics Recall\": round(avg_recall,3), \"Average of String Metrics F1\": round(avg_f1,3)}\n",
    "        \n",
    "    def get_string_semantic_metric_avg(self):\n",
    "        avg_precision = (list(self.avg_string_metric.values())[0] + list(self.fuzzy_semantics_sentences.values())[0]) / 2\n",
    "        avg_recall = (list(self.avg_string_metric.values())[1] + list(self.fuzzy_semantics_sentences.values())[1]) / 2\n",
    "        avg_f1 = (list(self.avg_string_metric.values())[2] + list(self.fuzzy_semantics_sentences.values())[2]) / 2\n",
    "        \n",
    "        return {\"Average of String and Semantics Metrics Precision\": round(avg_precision,3), \"Average of String and Semantics Metrics Recall\": round(avg_recall,3), \"Average of String and Semantics Metrics F1\": round(avg_f1,3)}\n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_dataframes_difference(self, dataframe_1, dataframe_2):\n",
    "        \"\"\"\n",
    "        This method compares two dataframes and returns the different cells.\n",
    "        \"\"\"\n",
    "        if dataframe_1.shape != dataframe_2.shape:\n",
    "            sys.stderr.write(\"Two compared datasets do not have equal sizes!\\n\")\n",
    "        difference_dictionary = {}\n",
    "        difference_dataframe = dataframe_1.where(dataframe_1.values != dataframe_2.values).notna()\n",
    "        for j in range(dataframe_1.shape[1]):\n",
    "            for i in difference_dataframe.index[difference_dataframe.iloc[:, j]].tolist():\n",
    "                difference_dictionary[(i, j)] = dataframe_2.iloc[i, j]\n",
    "        return difference_dictionary\n",
    "    \n",
    "    def get_error_corrected_val(self):\n",
    "        correction_dict = self.get_dataframes_difference(self.dirty_data, self.corrected_data)\n",
    "        for key in list(correction_dict):\n",
    "            if key not in self.error_clean_val:\n",
    "                del correction_dict[key]\n",
    "    \n",
    "        return correction_dict\n",
    "    \n",
    "  \n",
    "    def jaro_winkler_distance(self, s1, s2):\n",
    "        \"\"\"\n",
    "        Compute Jaro-Winkler distance between two strings.\n",
    "        \"\"\"\n",
    "        # If the s are equal\n",
    "        if (s1 == s2):\n",
    "            return 1.0\n",
    "\n",
    "        # Length of two s\n",
    "        len1 = len(s1)\n",
    "        len2 = len(s2)\n",
    "\n",
    "        # Maximum distance upto which matching\n",
    "        # is allowed\n",
    "        max_dist = floor(max(len1, len2) / 2) - 1\n",
    "\n",
    "        # Count of matches\n",
    "        match = 0\n",
    "\n",
    "        # Hash for matches\n",
    "        hash_s1 = [0] * len(s1)\n",
    "        hash_s2 = [0] * len(s2)\n",
    "\n",
    "        # Traverse through the first\n",
    "        for i in range(len1):\n",
    "\n",
    "            # Check if there is any matches\n",
    "            for j in range(max(0, i - max_dist), \n",
    "                           min(len2, i + max_dist + 1)):\n",
    "\n",
    "                # If there is a match\n",
    "                if (s1[i] == s2[j] and hash_s2[j] == 0):\n",
    "                    hash_s1[i] = 1\n",
    "                    hash_s2[j] = 1\n",
    "                    match += 1\n",
    "                    break\n",
    "\n",
    "        # If there is no match\n",
    "        if (match == 0):\n",
    "            return 0.0\n",
    "\n",
    "        # Number of transpositions\n",
    "        t = 0\n",
    "        point = 0\n",
    "\n",
    "        # Count number of occurrences\n",
    "        # where two characters match but\n",
    "        # there is a third matched character\n",
    "        # in between the indices\n",
    "        for i in range(len1):\n",
    "            if (hash_s1[i]):\n",
    "\n",
    "                # Find the next matched character\n",
    "                # in second\n",
    "                while (hash_s2[point] == 0):\n",
    "                    point += 1\n",
    "\n",
    "                if (s1[i] != s2[point]):\n",
    "                    t += 1\n",
    "                point += 1\n",
    "        t = t//2\n",
    "\n",
    "        # Return the Jaro Similarity\n",
    "        return (match/ len1 + match / len2 +\n",
    "                (match - t) / match)/ 3.0\n",
    "    \n",
    "\n",
    "    def jaro_winkler_distance_fuzzy(self, clean, dirty, corrected):\n",
    "        jw_clean_dirty = self.jaro_winkler_distance(clean, dirty)\n",
    "        jw_clean_corrected = self.jaro_winkler_distance(clean, corrected)\n",
    "\n",
    "\n",
    "        return jw_clean_corrected - jw_clean_dirty\n",
    "    \n",
    "    \n",
    "    def get_data_cleaning_evaluation_fuzzy_JW(self, num_metric=False):\n",
    "        \"\"\"\n",
    "        This method evaluates data cleaning process using fuzzy metrics\n",
    "        \"\"\"\n",
    "        pc_r = 0.0 #right partial corrections\n",
    "        pc_f = 0.0 #false partial corrections\n",
    "        ec_tp = 0.0\n",
    "        output_size = 0.0\n",
    "        \n",
    "        for cell in self.error_corrected_val:\n",
    "            output_size += 1\n",
    "            if self.error_corrected_val[cell] == self.error_clean_val[cell]:\n",
    "                ec_tp += 1.0\n",
    "            elif (num_metric and cell[1] in self.str_attr and cell[1] not in self.numer_attr) or (not num_metric and cell[1] in self.str_attr):\n",
    "                metric_score = self.jaro_winkler_distance_fuzzy(self.error_clean_val[cell], self.error_dirty_val[cell],  self.error_corrected_val[cell])\n",
    "                ec_tp += metric_score\n",
    "                print([self.error_clean_val[cell] ,  self.error_dirty_val[cell],  self.error_corrected_val[cell], metric_score])\n",
    "                if metric_score >= 0:\n",
    "                    pc_r += 1\n",
    "                else:\n",
    "                    pc_f += 1 \n",
    "        \n",
    "        \n",
    "        if num_metric:\n",
    "            ec_tp += self.numer_tp\n",
    "            ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "            ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "            ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "            return {\"Fuzzy JW Num Precision\": round(ec_p, 3),\"Fuzzy JW Num Recall\": round(ec_r, 3), \"Fuzzy JW Num F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "        else:\n",
    "            ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "            ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "            ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "            return {\"Fuzzy JW Precision\": round(ec_p, 3),\"Fuzzy JW Recall\": round(ec_r, 3), \"Fuzzy JW F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "\n",
    "    def get_data_cleaning_evaluation(self):\n",
    "        \"\"\"\n",
    "        This method evaluates data cleaning process using fuzzy metrics\n",
    "        \"\"\"\n",
    "        ec_tp = 0.0\n",
    "        output_size = 0.0\n",
    "        for cell in self.error_corrected_val:\n",
    "            output_size += 1\n",
    "            if self.error_corrected_val[cell] == self.error_clean_val[cell]:\n",
    "                ec_tp += 1.0\n",
    "        ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "        ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "        ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "        return {\"Precision\": round(ec_p, 3),\"Recall\": round(ec_r, 3), \"F1\": round(ec_f, 3), \"Amount of fixed data errors\": output_size}\n",
    "    \n",
    "    def get_single_outlier_score(self, clean, dirty, corrected):\n",
    "        score = 1 - (abs(int(clean)-int(corrected)) / abs(int(clean) - int(dirty)))\n",
    "    \n",
    "        if score >= 0:\n",
    "            return score\n",
    "\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    def get_fuzzy_score_outlier(self, clean, dirty, corrected):\n",
    "        clean = re.findall(r'\\d+', clean)\n",
    "        dirty = re.findall(r'\\d+', dirty)\n",
    "        corrected = re.findall(r'\\d+', corrected)\n",
    "\n",
    "        if len(clean) != len(dirty) or len(dirty) != len(corrected) or len(corrected) != len(clean):\n",
    "            return 0\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for (o,d,c) in  zip(clean, dirty, corrected):\n",
    "            count += self.get_single_outlier_score(o, d, c)\n",
    "\n",
    "        return count/len(clean)\n",
    "    \n",
    "    def get_numer_tp(self):\n",
    "        \"\"\"\n",
    "        This method evaluates data cleaning process using fuzzy metrics\n",
    "        \"\"\"\n",
    "\n",
    "        ec_tp = 0.0\n",
    "        for cell in self.error_corrected_val:\n",
    "            if cell in self.error_clean_val:\n",
    "                if cell[1] in self.numer_attr and self.error_corrected_val[cell] != self.error_clean_val[cell]:\n",
    "                    ec_tp += self.get_fuzzy_score_outlier(self.error_clean_val[cell], self.error_dirty_val[cell],  self.error_corrected_val[cell])\n",
    "                    \n",
    "        return ec_tp \n",
    "    \n",
    "    \n",
    "    def get_data_cleaning_evaluation_fuzzy_alt(self, num_metric=False):\n",
    "        \"\"\"\n",
    "        This method evaluates data cleaning process using fuzzy metrics\n",
    "        \"\"\"\n",
    "\n",
    "        ed_tp = 0.0\n",
    "        ec_tp = 0.0\n",
    "        output_size = 0.0\n",
    "        for cell in self.error_corrected_val:\n",
    "            output_size += 1\n",
    "            if cell in self.error_clean_val:\n",
    "                ed_tp += 1.0\n",
    "                if self.error_corrected_val[cell] == self.error_clean_val[cell]:\n",
    "                    ec_tp += 1.0\n",
    "                elif cell[1] in self.numer_attr and num_metric:\n",
    "                    ec_tp += self.get_fuzzy_score_outlier(self.error_clean_val[cell], self.error_dirty_val[cell],  self.error_corrected_val[cell])\n",
    "                elif cell[1] in self.str_attr:\n",
    "                    ec_tp += self.get_fuzzy_score_string_alt(self.error_corrected_val[cell], self.error_clean_val[cell], self.error_corrected_val[cell])\n",
    "        ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "        ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "        ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "        \n",
    "        if num_metric:\n",
    "            return {\"Fuzzy Alt Num Precision\": round(ec_p, 3),\"Fuzzy Alt Num Recall\": round(ec_r, 3), \"Fuzzy Alt Num F1\": round(ec_f, 3)}\n",
    "        else:\n",
    "            return {\"Fuzzy Alt Precision\": round(ec_p, 3),\"Fuzzy Alt Recall\": round(ec_r, 3), \"Fuzzy Alt F1\": round(ec_f, 3)}\n",
    "    \n",
    "    def get_fuzzy_score_string_alt(self, clean, dirty, corrected ):\n",
    "        if len(clean) != len(corrected) or len(dirty) != len(clean):\n",
    "            return 0\n",
    "\n",
    "        count_w = 0.0\n",
    "        count_r = 0.0\n",
    "        for o, c, d in zip(clean, corrected, dirty):\n",
    "            if o != d:\n",
    "                count_w += 1\n",
    "                if o == c:\n",
    "                    count_r += 1\n",
    "            else:\n",
    "                if c != d:\n",
    "                    count_r -= 1\n",
    "\n",
    "        if count_r <= 0:\n",
    "            return 0\n",
    "\n",
    "        return count_r / count_w\n",
    "    \n",
    "    def get_data_cleaning_evaluation_fuzzy_ME(self, num_metric=False):\n",
    "        \"\"\"\n",
    "        This method evaluates data cleaning process using fuzzy metrics\n",
    "        \"\"\"\n",
    "        pc_r = 0.0 #right partial corrections\n",
    "        pc_f = 0.0 #false partial corrections\n",
    "        \n",
    "        ec_tp = 0.0\n",
    "        output_size = 0.0\n",
    "        for cell in self.error_corrected_val:\n",
    "            output_size += 1\n",
    "            if cell in self.error_clean_val:\n",
    "                if self.error_corrected_val[cell] == self.error_clean_val[cell]:\n",
    "                    ec_tp += 1.0\n",
    "                elif (num_metric and cell[1] in self.str_attr and cell[1] not in self.numer_attr) or (not num_metric and cell[1] in self.str_attr):\n",
    "                    metric_score = self.monge_elkan_distance_fuzzy([self.error_corrected_val[cell]], [self.error_clean_val[cell]], [self.error_dirty_val[cell]])\n",
    "                    ec_tp += metric_score\n",
    "                    if metric_score >= 0:\n",
    "                        pc_r += 1\n",
    "                    else:\n",
    "                        pc_f += 1\n",
    "    \n",
    "        if num_metric:\n",
    "            ec_tp += self.numer_tp\n",
    "            ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "            ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "            ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "            return {\"Fuzzy ME Num Precision\": round(ec_p, 3),\"Fuzzy ME Num Recall\": round(ec_r, 3), \"Fuzzy ME Num F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "        else:\n",
    "            ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "            ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "            ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "            return {\"Fuzzy ME Precision\": round(ec_p, 3),\"Fuzzy ME Recall\": round(ec_r, 3), \"Fuzzy ME F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def monge_elkan(self, bag1, bag2):\n",
    "        \"\"\"\n",
    "        Compute Monge-Elkan similarity measure between two bags (lists).\n",
    "\n",
    "        The Monge-Elkan similarity measure is a type of Hybrid similarity measure that combine the benefits of\n",
    "        sequence-based and set-based methods. This can be effective for domains in which more control is needed\n",
    "        over the similarity measure. It implicitly uses a secondary similarity measure, such as levenshtein to compute\n",
    "        over all similarity score.\n",
    "\n",
    "        Args:\n",
    "            bag1,bag2 (list): Input lists\n",
    "\n",
    "            sim_func (function): Secondary similarity function. This is expected to be a sequence-based\n",
    "                similarity measure (defaults to levenshtein)\n",
    "\n",
    "        Returns:\n",
    "            Monge-Elkan similarity score (float)\n",
    "\n",
    "        Raises:\n",
    "            TypeError : If the inputs are not lists or if one of the inputs is None\n",
    "\n",
    "\n",
    "        Examples:\n",
    "            >>> monge_elkan(['Niall'], ['Neal'])\n",
    "            0.8049999999999999\n",
    "            >>> monge_elkan(['Comput.', 'Sci.', 'and', 'Eng.', 'Dept.,', 'University', 'of', 'California,', 'San', 'Diego'], ['Department', 'of', 'Computer', 'Science,', 'Univ.', 'Calif.,', 'San', 'Diego'])\n",
    "            0.8677218614718616\n",
    "            >>> monge_elkan(['Comput.', 'Sci.', 'and', 'Eng.', 'Dept.,', 'University', 'of', 'California,', 'San', 'Diego'], ['Department', 'of', 'Computer', 'Science,', 'Univ.', 'Calif.,', 'San', 'Diego'], sim_func=needleman_wunsch)\n",
    "            2.0\n",
    "            >>> monge_elkan(['Comput.', 'Sci.', 'and', 'Eng.', 'Dept.,', 'University', 'of', 'California,', 'San', 'Diego'], ['Department', 'of', 'Computer', 'Science,', 'Univ.', 'Calif.,', 'San', 'Diego'], sim_func=affine)\n",
    "            2.25\n",
    "            >>> monge_elkan([''], ['a'])\n",
    "            0.0\n",
    "            >>> monge_elkan(['Niall'], ['Nigel'])\n",
    "            0.7866666666666667\n",
    "\n",
    "        References:\n",
    "            * Principles of Data Integration book\n",
    "        \"\"\"\n",
    "\n",
    "        # if exact match return 1.0\n",
    "        if bag1 == bag2:\n",
    "            return 1.0\n",
    "        # if one of the strings is empty return 0\n",
    "        if (len(bag1) == 0) or (len(bag2) == 0):\n",
    "            return 0\n",
    "        # aggregated sum of all the max sim score of all the elements in bag1\n",
    "        # with elements in bag2\n",
    "        sum_of_maxes = 0\n",
    "        for t1 in bag1:\n",
    "            max_sim = float('-inf')\n",
    "            for t2 in bag2:\n",
    "                max_sim = max(max_sim, self.jaro_winkler_distance(t1, t2))\n",
    "            sum_of_maxes += max_sim\n",
    "        sim = float(sum_of_maxes) / float(len(bag1))\n",
    "        return sim\n",
    "\n",
    "\n",
    "    def monge_elkan_distance_fuzzy(self, clean, dirty, corrected):\n",
    "        me_clean_dirty = self.monge_elkan(clean, dirty)\n",
    "        me_clean_corrected = self.monge_elkan(clean, corrected)\n",
    "        #print(monge_elkan(clean, dirty))\n",
    "        #print(monge_elkan(clean, corrected))\n",
    "\n",
    "\n",
    "        return me_clean_corrected - me_clean_dirty\n",
    "        \n",
    "    def get_data_cleaning_evaluation_fuzzy_LD(self, num_metric=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        This method evaluates data cleaning process using fuzzy metrics\n",
    "        \"\"\"\n",
    "        pc_r = 0.0 #right partial corrections\n",
    "        pc_f = 0.0 #false partial corrections\n",
    "        ed_tp = 0.0\n",
    "        ec_tp = 0.0\n",
    "        output_size = 0.0\n",
    "        for cell in self.error_corrected_val:\n",
    "            output_size += 1\n",
    "            if cell in self.error_clean_val:\n",
    "                ed_tp += 1.0\n",
    "                if self.error_corrected_val[cell] == self.error_clean_val[cell]:\n",
    "                    ec_tp += 1.0\n",
    "                    \n",
    "                elif cell[1] in self.numer_attr and num_metric:\n",
    "                    ec_tp += self.get_fuzzy_score_outlier(self.error_clean_val[cell], self.error_dirty_val[cell],  self.error_corrected_val[cell])\n",
    "                    \n",
    "                elif cell[1] in self.long_str_attr:\n",
    "                    metric_score = self.fuzzy_LD_Words(self.error_clean_val[cell], self.error_dirty_val[cell],  self.error_corrected_val[cell])       \n",
    "                    ec_tp += metric_score\n",
    "                    if metric_score >= 0:\n",
    "                        pc_r += 1\n",
    "                    else:\n",
    "                        pc_f += 1\n",
    "                        \n",
    "                elif cell[1] in self.short_str_attr:\n",
    "                    metric_score = self.fuzzy_LD_Char(self.error_clean_val[cell], self.error_dirty_val[cell],  self.error_corrected_val[cell])\n",
    "                    ec_tp += metric_score\n",
    "                    if metric_score >= 0:\n",
    "                        pc_r += 1\n",
    "                    else:\n",
    "                        pc_f += 1\n",
    "                        \n",
    "        ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "        ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "        ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "        \n",
    "        if num_metric:\n",
    "            return {\"Fuzzy LD Num Precision\": round(ec_p, 3),\"Fuzzy LD Recall\": round(ec_r, 3), \"Fuzzy LD F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "        else:\n",
    "            return {\"Fuzzy LD Precision\": round(ec_p, 3),\"Fuzzy LD Recall\": round(ec_r, 3), \"Fuzzy LD F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "    \n",
    "\n",
    "    def get_data_cleaning_evaluation_fuzzy_LD_Char(self, num_metric=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        This method evaluates data cleaning process using fuzzy metrics\n",
    "        \"\"\"\n",
    "        pc_r = 0.0 #right partial corrections\n",
    "        pc_f = 0.0 #false partial corrections\n",
    "        \n",
    "        ec_tp = 0.0\n",
    "        output_size = 0.0\n",
    "        for cell in self.error_corrected_val:\n",
    "            output_size += 1\n",
    "            if self.error_corrected_val[cell] == self.error_clean_val[cell]:\n",
    "                ec_tp += 1.0\n",
    "            elif (num_metric and cell[1] in self.str_attr and cell[1] not in self.numer_attr) or (not num_metric and cell[1] in self.str_attr):\n",
    "                metric_score = self.fuzzy_LD_Char(self.error_clean_val[cell], self.error_dirty_val[cell],  self.error_corrected_val[cell])\n",
    "                ec_tp += metric_score\n",
    "                #print([self.error_clean_val[cell] ,  self.error_dirty_val[cell],  self.error_corrected_val[cell], metric_score])\n",
    "                if metric_score >= 0:\n",
    "                    pc_r += 1\n",
    "                else:\n",
    "                    pc_f += 1\n",
    "        ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "        ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "        ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "        \n",
    "        if num_metric:\n",
    "            ec_tp += self.numer_tp\n",
    "            ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "            ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "            ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "            return {\"Fuzzy LD Char Num Precision\": round(ec_p, 3),\"Fuzzy LD Char Num Recall\": round(ec_r, 3), \"Fuzzy LD Char Num F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "        else: \n",
    "            ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "            ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "            ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "            return {\"Fuzzy LD Char Precision\": round(ec_p, 3),\"Fuzzy LD Char Recall\": round(ec_r, 3), \"Fuzzy LD Char F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "    \n",
    "    def get_data_cleaning_evaluation_fuzzy_LD_Words(self, num_metric=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        This method evaluates data cleaning process using fuzzy metrics\n",
    "        \"\"\"\n",
    "        pc_r = 0.0 #right partial corrections\n",
    "        pc_f = 0.0 #false partial corrections\n",
    "        ed_tp = 0.0\n",
    "        ec_tp = 0.0\n",
    "        output_size = 0.0\n",
    "        for cell in self.error_corrected_val:\n",
    "            output_size += 1\n",
    "            if cell in self.error_clean_val:\n",
    "                ed_tp += 1.0\n",
    "                if self.error_corrected_val[cell] == self.error_clean_val[cell]:\n",
    "                    ec_tp += 1.0\n",
    "                elif (num_metric and cell[1] in self.str_attr and cell[1] not in self.numer_attr) or (not num_metric and cell[1] in self.str_attr):\n",
    "                    metric_score = self.fuzzy_LD_Words(self.error_clean_val[cell], self.error_dirty_val[cell],  self.error_corrected_val[cell])       \n",
    "                    ec_tp += metric_score\n",
    "                    if metric_score >= 0:\n",
    "                        pc_r += 1\n",
    "                    else:\n",
    "                        pc_f += 1\n",
    "        \n",
    "        if num_metric:\n",
    "            ec_tp += self.numer_tp\n",
    "            ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "            ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "            ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "            return {\"Fuzzy LD Words Num Precision\": round(ec_p, 3),\"Fuzzy LD Words Num Recall\": round(ec_r, 3), \"Fuzzy LD Words Num F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "        else:\n",
    "            ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "            ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "            ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "            return {\"Fuzzy LD Words Precision\": round(ec_p, 3),\"Fuzzy LD Words Recall\": round(ec_r, 3), \"Fuzzy LD Words F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "\n",
    "\n",
    "    def fuzzy_LD_Char(self, clean, dirty, corrected):\n",
    "        LD_clean_dirty =  self.levenshteinDistanceChar(clean, dirty)\n",
    "        LD_clean_corrected = self.levenshteinDistanceChar(clean, corrected)\n",
    "        print([clean, dirty, corrected, LD_clean_dirty, LD_clean_corrected,(LD_clean_corrected-LD_clean_dirty) ])\n",
    "\n",
    "        return (LD_clean_corrected-LD_clean_dirty)\n",
    "    \n",
    "    \n",
    "    def fuzzy_LD_Words(self, clean, dirty, corrected):\n",
    "        LD_clean_dirty =  self.levenshteinDistanceWords(clean, dirty)\n",
    "        LD_clean_corrected = self.levenshteinDistanceWords(clean, corrected, (LD_clean_dirty - LD_clean_corrected) / max([len(clean), len(dirty)]))\n",
    "\n",
    "       \n",
    "        return (LD_clean_dirty - LD_clean_corrected) / max([len(clean), len(dirty)])\n",
    "        \n",
    "    def levenshteinDistanceWords(self, token1, token2):\n",
    "        token1 = token1.split()\n",
    "        token2 = token2.split()\n",
    "        distances = numpy.zeros((len(token1) + 1, len(token2) + 1))\n",
    "\n",
    "        for t1 in range(len(token1) + 1):\n",
    "            distances[t1][0] = t1\n",
    "\n",
    "        for t2 in range(len(token2) + 1):\n",
    "            distances[0][t2] = t2\n",
    "\n",
    "        a = 0\n",
    "        b = 0\n",
    "        c = 0\n",
    "\n",
    "        for t1 in range(1, len(token1) + 1):\n",
    "            for t2 in range(1, len(token2) + 1):\n",
    "                if (token1[t1-1] == token2[t2-1]):\n",
    "                    distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "                else:\n",
    "                    a = distances[t1][t2 - 1]\n",
    "                    b = distances[t1 - 1][t2]\n",
    "                    c = distances[t1 - 1][t2 - 1]\n",
    "\n",
    "                    if (a <= b and a <= c):\n",
    "                        distances[t1][t2] = a + 1\n",
    "                    elif (b <= a and b <= c):\n",
    "                        distances[t1][t2] = b + 1\n",
    "                    else:\n",
    "                        distances[t1][t2] = c + 1\n",
    "\n",
    "\n",
    "        return distances[len(token1)][len(token2)]\n",
    "\n",
    "    def levenshteinDistanceChar(self, token1, token2):\n",
    "        distances = numpy.zeros((len(token1) + 1, len(token2) + 1))\n",
    "\n",
    "        for t1 in range(len(token1) + 1):\n",
    "            distances[t1][0] = t1\n",
    "\n",
    "        for t2 in range(len(token2) + 1):\n",
    "            distances[0][t2] = t2\n",
    "\n",
    "        a = 0\n",
    "        b = 0\n",
    "        c = 0\n",
    "\n",
    "        for t1 in range(1, len(token1) + 1):\n",
    "            for t2 in range(1, len(token2) + 1):\n",
    "                if (token1[t1-1] == token2[t2-1]):\n",
    "                    distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "                else:\n",
    "                    a = distances[t1][t2 - 1]\n",
    "                    b = distances[t1 - 1][t2]\n",
    "                    c = distances[t1 - 1][t2 - 1]\n",
    "\n",
    "                    if (a <= b and a <= c):\n",
    "                        distances[t1][t2] = a + 1\n",
    "                    elif (b <= a and b <= c):\n",
    "                        distances[t1][t2] = b + 1\n",
    "                    else:\n",
    "                        distances[t1][t2] = c + 1\n",
    "\n",
    "\n",
    "        return 1 - ((distances[len(token1)][len(token2)])/max([len(token1), len(token2)]))\n",
    "    \n",
    "    def get_fuzzy_score_outlier(self, clean, dirty, corrected):\n",
    "        clean = re.findall(r'\\d+', clean)\n",
    "        dirty = re.findall(r'\\d+', dirty)\n",
    "        corrected = re.findall(r'\\d+', corrected)\n",
    "\n",
    "        if len(clean) != len(dirty) or len(dirty) != len(corrected) or len(corrected) != len(clean):\n",
    "            return 0\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        #for (o,d,c) in  zip(clean, dirty, corrected):\n",
    "        #    if (int(o) > int(d) and int(d) > int(c)) or (int(o) <int(d) and int(d) < int(c)):\n",
    "        #        return 0\n",
    "        #    if abs(int(o) - int(c)) < abs(int(o) - int(d)):\n",
    "        #        count += abs(int(o)-int(c)) / abs(int(o) - int(d))\n",
    "        \n",
    "        for (o,d,c) in  zip(clean, dirty, corrected):\n",
    "            if abs(int(o) - int(d)) != 0:\n",
    "                count += abs(int(o)-int(c)) / abs(int(o) - int(d))\n",
    "            if abs(int(o) - int(c)) < abs(int(o) - int(d)):\n",
    "                count += abs(int(o)-int(c)) / abs(int(o) - int(d))\n",
    "       \n",
    "        return count/len(clean)\n",
    "    \n",
    "    \n",
    "    def get_fuzzy_score_semantic_sentence(self, clean, dirty, corrected):\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "        embeddings1 = model.encode(clean, convert_to_tensor=True)\n",
    "        embeddings2 = model.encode(dirty, convert_to_tensor=True)\n",
    "        embeddings3 = model.encode(corrected, convert_to_tensor=True)\n",
    "\n",
    "        cosine_scores_clean_dirty = util.cos_sim(embeddings1, embeddings2)\n",
    "        cosine_scores_clean_corrected = util.cos_sim(embeddings1, embeddings3)\n",
    "\n",
    "\n",
    "        return (cosine_scores_clean_corrected[0][0].item() - 0.2) / (0.8) - (cosine_scores_clean_dirty[0][0].item() - 0.2) / (0.8)\n",
    "    \n",
    "    def get_semantic_score(self, s1, s2):\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "        embeddings1 = model.encode(s1, convert_to_tensor=True)\n",
    "        embeddings2 = model.encode(s2, convert_to_tensor=True)\n",
    "        score = util.cos_sim(embeddings1, embeddings2)\n",
    "        \n",
    "        return (score[0][0].item() - 0.2) / (0.8)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def get_data_cleaning_evaluation_fuzzy_semantic_sentences(self, num_metric=False):\n",
    "\n",
    "        \"\"\"\n",
    "        This method evaluates data cleaning process using fuzzy metrics\n",
    "        \"\"\"\n",
    "\n",
    "        pc_r = 0.0 #right partial corrections\n",
    "        pc_f = 0.0 #false partial corrections\n",
    "        ec_tp = 0.0\n",
    "        output_size = 0.0\n",
    "        for cell in self.error_corrected_val:\n",
    "            output_size += 1\n",
    "            if cell in self.error_clean_val:\n",
    "                if self.error_corrected_val[cell] == self.error_clean_val[cell]:\n",
    "                    ec_tp += 1.0\n",
    "                elif (num_metric and cell[1] in self.str_attr and cell[1] not in self.numer_attr) or (not num_metric and cell[1] in self.str_attr):\n",
    "                    metric_score = self.get_fuzzy_score_semantic_sentence(self.error_clean_val[cell], self.error_dirty_val[cell],  self.error_corrected_val[cell])\n",
    "                    ec_tp += metric_score\n",
    "                    if metric_score >= 0:\n",
    "                        pc_r += 1\n",
    "                    else:\n",
    "                        pc_f += 1\n",
    "\n",
    "        if num_metric:\n",
    "            ec_tp += self.numer_tp\n",
    "            ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "            ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "            ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "            return {\"Fuzzy Semantic Sentences Num Precision\": round(ec_p, 3),\"Fuzzy Semantic Sentences Num Recall\": round(ec_r, 3), \"Fuzzy Semantic Sentences Num F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "        else:    \n",
    "            ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "            ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "            ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "            return {\"Fuzzy Semantic Sentences Precision\": round(ec_p, 3),\"Fuzzy Semantic Sentences Recall\": round(ec_r, 3), \"Fuzzy Semantic Sentences F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "        \n",
    "        \n",
    "    def get_combined_score_evaluation(self, num_metric=False):\n",
    "        pc_r = 0.0 #right partial corrections\n",
    "        pc_f = 0.0 #false partial corrections\n",
    "        ec_tp = 0.0\n",
    "        output_size = 0.0\n",
    "        for cell in self.error_corrected_val:\n",
    "            output_size += 1\n",
    "            if cell in self.error_clean_val:\n",
    "                if self.error_corrected_val[cell] == self.error_clean_val[cell]:\n",
    "                    ec_tp += 1.0\n",
    "                elif (num_metric and cell[1] in self.str_attr and cell[1] not in self.numer_attr) or (not num_metric and cell[1] in self.str_attr):\n",
    "                    \n",
    "                    clean_dirty_semantic_score = self.get_semantic_score(self.error_clean_val[cell], self.error_dirty_val[cell])\n",
    "                    clean_dirty_string_score = self.get_string_score_avg(self.error_clean_val[cell], self.error_dirty_val[cell])\n",
    "                    clean_corrected_semantic_score = self.get_semantic_score(self.error_clean_val[cell], self.error_corrected_val[cell])\n",
    "                    clean_corrected_string_score = self.get_string_score_avg(self.error_clean_val[cell], self.error_corrected_val[cell])\n",
    "                    \n",
    "                    combined_score = self.get_combined_score(clean_dirty_semantic_score, clean_dirty_string_score, clean_corrected_semantic_score, clean_corrected_string_score)\n",
    "                    \n",
    "                    ec_tp += combined_score\n",
    "                    if combined_score >= 0:\n",
    "                        pc_r += 1\n",
    "                    else:\n",
    "                        pc_f += 1\n",
    "                        \n",
    "        ec_p = 0.0 if output_size == 0 else ec_tp / output_size\n",
    "        ec_r = 0.0 if len(self.error_clean_val) == 0 else ec_tp / len(self.error_clean_val)\n",
    "        ec_f = 0.0 if (ec_p + ec_r) == 0.0 else (2 * ec_p * ec_r) / (ec_p + ec_r)\n",
    "        return {\"Combined Precision\": round(ec_p, 3),\"Combined Recall\": round(ec_r, 3), \"Combined F1\": round(ec_f, 3), \"PC R\": pc_r, \"PC F\": pc_f}\n",
    "                        \n",
    "                        \n",
    "    def get_string_score_avg(self, s1, s2):\n",
    "        score_avg = ((self.levenshteinDistanceChar(s1, s2) / max([len(s1), len(s2)])) + self.monge_elkan(s1, s2) + self.jaro_winkler_distance(s1, s2)) / 3\n",
    "        return score_avg\n",
    "    \n",
    "    def get_combined_score(self, cd_semantic, cd_string, cc_semantic, cc_string):\n",
    "        \n",
    "        threshold = 0.7\n",
    "        \n",
    "        string_score = cc_string - cd_string\n",
    "        semantic_score = cc_semantic - cd_semantic\n",
    "        avg_score = (string_score + semantic_score) / 2\n",
    "        \n",
    "        \n",
    "        #semantic score high and string score high\n",
    "        if cd_semantic >= threshold and cc_string >=threshold:\n",
    "            \n",
    "            if cc_semantic >= threshold and cc_string >=threshold:\n",
    "                return avg_score\n",
    "            \n",
    "            elif cc_semantic >= threshold and cc_string < threshold:\n",
    "                return string_score\n",
    "\n",
    "            elif cc_semantic < threshold and cc_string >= threshold:\n",
    "                return semantic_score\n",
    "\n",
    "            elif cc_semantic < threshold and cc_string < threshold:\n",
    "                return avg_score\n",
    "        \n",
    "        \n",
    "        #semantic score high and string score low\n",
    "        elif cd_semantic >= threshold and cd_string < threshold:\n",
    "            \n",
    "            if cc_semantic >= threshold and cc_string >=threshold:\n",
    "                return string_score\n",
    "            \n",
    "            elif cc_semantic >= threshold and cc_string < threshold:\n",
    "                return avg_score\n",
    "                \n",
    "            elif cc_semantic < threshold and cc_string >= threshold:\n",
    "                return avg_score \n",
    "\n",
    "            elif cc_semantic < threshold and cc_string < threshold:\n",
    "                return semantic_score\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #semantic score low and string score high\n",
    "        elif cd_semantic < threshold and cd_string >= threshold:\n",
    "            \n",
    "            if cc_semantic >= threshold and cc_string >=threshold:\n",
    "                return semantic_score\n",
    "            \n",
    "            elif cc_semantic >= threshold and cc_string < threshold:\n",
    "                return avg_score \n",
    "\n",
    "            elif cc_semantic < threshold and cc_string >= threshold:\n",
    "                return avg_score \n",
    "\n",
    "            elif cc_semantic < threshold and cc_string < threshold:\n",
    "                return string_score\n",
    "        \n",
    "        \n",
    "        #semantic score low and string score low\n",
    "        elif cd_semantic < threshold and cd_string < threshold:\n",
    "            \n",
    "            if cc_semantic >= threshold and cc_string >=threshold:\n",
    "                return avg_score\n",
    "            \n",
    "            elif cc_semantic >= threshold and cc_string < threshold:\n",
    "                return semantic_score\n",
    "\n",
    "            elif cc_semantic < threshold and cc_string >= threshold:\n",
    "                return string_score\n",
    "\n",
    "            elif cc_semantic < threshold and cc_string < threshold:\n",
    "                return avg_score\n",
    "            \n",
    "        return avg_score\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b12018cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1/7/01', '1/1/07', '', -0.7777777777777777]\n",
      "['1/7/01', '1/1/07', '', -0.7777777777777777]\n",
      "['1/4/01', '1/1/04', '', -0.7777777777777777]\n",
      "['1/7/01', '1/1/07', '', -0.7777777777777777]\n",
      "['1/12/01', '1/1/12', '', -0.8968253968253969]\n",
      "['1/7/01', '1/1/07', '', -0.7777777777777777]\n",
      "['1/12/01', '1/1/12', '3/1/99', -0.3650793650793651]\n",
      "['1/4/01', '1/1/04', '', -0.7777777777777777]\n",
      "['1/12/01', '1/1/12', '1/1/95', -0.1507936507936508]\n",
      "['1/3/06', '6/1/03', '4/1/14', -0.13888888888888884]\n",
      "['1/11/01', '1/1/11', '', -0.8968253968253969]\n",
      "['1/11/01', '1/1/11', '9/1/04', -0.23412698412698418]\n",
      "['{\"R Carraro\",\"M F Rasmussen\",\"L Niskanen\",\"A Harper\",\"N Finer\",\"S R_ssner\",\"M Kunesova\",\"L Van Gaal\",\"M E J Lean\",\"A Astrup\",\"A Rissanen\",\"M J Savolainen\",\"[Collective Name] NN8022-1807 Investigators\"}', '{\"R Carraro\",\"M F Rasmussen\",\"L Niskanen\",\"A Harper\",\"N Finer\",\"S RÌ¦ssner\",\"M Kunesova\",\"L Van Gaal\",\"M E J Lean\",\"A Astrup\",\"A Rissanen\",\"M J Savolainen\",\"[Collective Name] NN8022-1807 Investigators\"}', '{\"M F Jumean\",\"P J Erwin\",\"V M Montori\",\"F Lopez-Jimenez\",\"A Romero-Corral\",\"V K Somers\",\"D O Okorodudu\"}', -0.38724869561384656]\n",
      "['1/7/01', '1/1/07', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/7/01', '1/1/07', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/4/01', '1/1/04', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/7/01', '1/1/07', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/12/01', '1/1/12', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/7/01', '1/1/07', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/12/01', '1/1/12', '3/1/99', 0.5714285714285714, 0.4285714285714286, -0.1428571428571428]\n",
      "['1/4/01', '1/1/04', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/12/01', '1/1/12', '1/1/95', 0.5714285714285714, 0.5714285714285714, 0.0]\n",
      "['1/3/06', '6/1/03', '4/1/14', 0.5, 0.33333333333333337, -0.16666666666666663]\n",
      "['1/11/01', '1/1/11', '', 0.7142857142857143, 0.0, -0.7142857142857143]\n",
      "['1/11/01', '1/1/11', '9/1/04', 0.7142857142857143, 0.5714285714285714, -0.1428571428571429]\n",
      "['{\"R Carraro\",\"M F Rasmussen\",\"L Niskanen\",\"A Harper\",\"N Finer\",\"S R_ssner\",\"M Kunesova\",\"L Van Gaal\",\"M E J Lean\",\"A Astrup\",\"A Rissanen\",\"M J Savolainen\",\"[Collective Name] NN8022-1807 Investigators\"}', '{\"R Carraro\",\"M F Rasmussen\",\"L Niskanen\",\"A Harper\",\"N Finer\",\"S RÌ¦ssner\",\"M Kunesova\",\"L Van Gaal\",\"M E J Lean\",\"A Astrup\",\"A Rissanen\",\"M J Savolainen\",\"[Collective Name] NN8022-1807 Investigators\"}', '{\"M F Jumean\",\"P J Erwin\",\"V M Montori\",\"F Lopez-Jimenez\",\"A Romero-Corral\",\"V K Somers\",\"D O Okorodudu\"}', 0.9950248756218906, 0.24875621890547261, -0.746268656716418]\n",
      "['1/7/01', '1/1/07', '', -0.7777777777777777]\n",
      "['1/7/01', '1/1/07', '', -0.7777777777777777]\n",
      "['1/4/01', '1/1/04', '', -0.7777777777777777]\n",
      "['1/7/01', '1/1/07', '', -0.7777777777777777]\n",
      "['1/12/01', '1/1/12', '', -0.8968253968253969]\n",
      "['1/7/01', '1/1/07', '', -0.7777777777777777]\n",
      "['1/12/01', '1/1/12', '3/1/99', -0.3650793650793651]\n",
      "['1/4/01', '1/1/04', '', -0.7777777777777777]\n",
      "['1/12/01', '1/1/12', '1/1/95', -0.1507936507936508]\n",
      "['1/3/06', '6/1/03', '4/1/14', -0.13888888888888884]\n",
      "['1/11/01', '1/1/11', '', -0.8968253968253969]\n",
      "['1/11/01', '1/1/11', '9/1/04', -0.23412698412698418]\n",
      "['{\"R Carraro\",\"M F Rasmussen\",\"L Niskanen\",\"A Harper\",\"N Finer\",\"S R_ssner\",\"M Kunesova\",\"L Van Gaal\",\"M E J Lean\",\"A Astrup\",\"A Rissanen\",\"M J Savolainen\",\"[Collective Name] NN8022-1807 Investigators\"}', '{\"R Carraro\",\"M F Rasmussen\",\"L Niskanen\",\"A Harper\",\"N Finer\",\"S RÌ¦ssner\",\"M Kunesova\",\"L Van Gaal\",\"M E J Lean\",\"A Astrup\",\"A Rissanen\",\"M J Savolainen\",\"[Collective Name] NN8022-1807 Investigators\"}', '{\"M F Jumean\",\"P J Erwin\",\"V M Montori\",\"F Lopez-Jimenez\",\"A Romero-Corral\",\"V K Somers\",\"D O Okorodudu\"}', -0.38724869561384656]\n",
      "['1/7/01', '1/1/07', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/7/01', '1/1/07', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/4/01', '1/1/04', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/7/01', '1/1/07', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/12/01', '1/1/12', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/7/01', '1/1/07', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/12/01', '1/1/12', '3/1/99', 0.5714285714285714, 0.4285714285714286, -0.1428571428571428]\n",
      "['1/4/01', '1/1/04', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/12/01', '1/1/12', '1/1/95', 0.5714285714285714, 0.5714285714285714, 0.0]\n",
      "['1/3/06', '6/1/03', '4/1/14', 0.5, 0.33333333333333337, -0.16666666666666663]\n",
      "['1/11/01', '1/1/11', '', 0.7142857142857143, 0.0, -0.7142857142857143]\n",
      "['1/11/01', '1/1/11', '9/1/04', 0.7142857142857143, 0.5714285714285714, -0.1428571428571429]\n",
      "['{\"R Carraro\",\"M F Rasmussen\",\"L Niskanen\",\"A Harper\",\"N Finer\",\"S R_ssner\",\"M Kunesova\",\"L Van Gaal\",\"M E J Lean\",\"A Astrup\",\"A Rissanen\",\"M J Savolainen\",\"[Collective Name] NN8022-1807 Investigators\"}', '{\"R Carraro\",\"M F Rasmussen\",\"L Niskanen\",\"A Harper\",\"N Finer\",\"S RÌ¦ssner\",\"M Kunesova\",\"L Van Gaal\",\"M E J Lean\",\"A Astrup\",\"A Rissanen\",\"M J Savolainen\",\"[Collective Name] NN8022-1807 Investigators\"}', '{\"M F Jumean\",\"P J Erwin\",\"V M Montori\",\"F Lopez-Jimenez\",\"A Romero-Corral\",\"V K Somers\",\"D O Okorodudu\"}', 0.9950248756218906, 0.24875621890547261, -0.746268656716418]\n",
      "{'Combined Precision': 0.769, 'Combined Recall': 0.232, 'Combined F1': 0.356, 'PC R': 0.0, 'PC F': 13.0}\n",
      "\n",
      "{'Precision': 0.804, 'Recall': 0.243, 'F1': 0.373, 'Amount of fixed data errors': 285.0}\n",
      "{'Fuzzy JW Precision': 0.776, 'Fuzzy JW Recall': 0.234, 'Fuzzy JW F1': 0.36, 'PC R': 0.0, 'PC F': 13.0}\n",
      "{'Fuzzy ME Precision': 0.803, 'Fuzzy ME Recall': 0.243, 'Fuzzy ME F1': 0.373, 'PC R': 10.0, 'PC F': 3.0}\n",
      "{'Fuzzy LD Char Precision': 0.781, 'Fuzzy LD Char Recall': 0.236, 'Fuzzy LD Char F1': 0.362, 'PC R': 1.0, 'PC F': 12.0}\n",
      "{'Fuzzy Semantic Sentences Precision': 0.769, 'Fuzzy Semantic Sentences Recall': 0.232, 'Fuzzy Semantic Sentences F1': 0.356, 'PC R': 0.0, 'PC F': 13.0}\n",
      "\n",
      "{'Fuzzy JW Num Precision': 0.776, 'Fuzzy JW Num Recall': 0.234, 'Fuzzy JW Num F1': 0.36, 'PC R': 0.0, 'PC F': 13.0}\n",
      "{'Fuzzy ME Num Precision': 0.803, 'Fuzzy ME Num Recall': 0.243, 'Fuzzy ME Num F1': 0.373, 'PC R': 10.0, 'PC F': 3.0}\n",
      "{'Fuzzy LD Char Num Precision': 0.781, 'Fuzzy LD Char Num Recall': 0.236, 'Fuzzy LD Char Num F1': 0.362, 'PC R': 1.0, 'PC F': 12.0}\n",
      "{'Fuzzy Semantic Sentences Num Precision': 0.769, 'Fuzzy Semantic Sentences Num Recall': 0.232, 'Fuzzy Semantic Sentences Num F1': 0.356, 'PC R': 0.0, 'PC F': 13.0}\n"
     ]
    }
   ],
   "source": [
    "metric_dict_rayyan1 = {\n",
    "    \"clean_data_path\": \"../datasets/rayyan/clean.csv\",\n",
    "    \"dirty_data_path\": \"../datasets/rayyan/dirty.csv\",\n",
    "    \"corrected_data_path\": \"../datasets/rayyan/baran_repaired1.csv\",\n",
    "    \"str_attr\": [1, 2, 3, 4, 8, 10],\n",
    "    \"short_str_attr\": [2, 4, 8],\n",
    "    \"long_str_attr\": [1, 3, 10],\n",
    "    \"numer_attr\": [6, 7]\n",
    "    \n",
    "}\n",
    "\n",
    "m_rayyan1 = Metrics(metric_dict_rayyan1)\n",
    "\n",
    "m_rayyan1.print_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bea19096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4/14/07', '7/4/14', '', -0.7158730158730159]\n",
      "['1/14/07', '7/1/14', '', -0.7158730158730159]\n",
      "['1/12/01', '1/1/12', '', -0.8968253968253969]\n",
      "['1/10/01', '1/1/10', '', -0.8968253968253969]\n",
      "['2/15/04', '4/2/15', '', -0.7158730158730159]\n",
      "['1/14/11', '11/1/14', '', -0.7936507936507936]\n",
      "['1/5/04', '4/1/05', '', -0.6944444444444443]\n",
      "['1/12/01', '1/1/12', '', -0.8968253968253969]\n",
      "['1/10/10', '10/1/10', '', -0.9523809523809524]\n",
      "['1/15/01', '1/1/15', '', -0.8968253968253969]\n",
      "['1/10/01', '1/1/10', '', -0.8968253968253969]\n",
      "['1/12/01', '1/1/12', '', -0.8968253968253969]\n",
      "['1/11/01', '1/1/11', '', -0.8968253968253969]\n",
      "['9/15/10', '10/9/15', '', -0.6309523809523809]\n",
      "['1/5/01', '1/1/05', '', -0.7777777777777777]\n",
      "['1/11/01', '1/1/11', '', -0.8968253968253969]\n",
      "['4/15/11', '11/4/15', '', -0.6309523809523809]\n",
      "['1/12/01', '1/1/12', '', -0.8968253968253969]\n",
      "['1/11/01', '1/1/11', '', -0.8968253968253969]\n",
      "['1/11/01', '1/1/11', '', -0.8968253968253969]\n",
      "['10/15/09', '9/10/15', '', -0.7023809523809524]\n",
      "['{\"N_ria Lloberas\",\"Marcel la Franquesa\",\"Josep M Cruzado\",\"Josep M Griny_\",\"Ins Rama\",\"Gabriela Alperovich\",\"Immaculada Herrero-Fresneda\",\"Joan Torras\",\"Pepita Gimnez-Bonaf\"}', '{\"NÌ¼ria Lloberas\",\"Marcel la Franquesa\",\"Josep M Cruzado\",\"Josep M Grinyï¿½_\",\"InÌ©s Rama\",\"Gabriela Alperovich\",\"Immaculada Herrero-Fresneda\",\"Joan Torras\",\"Pepita GimÌ©nez-BonafÌ©\"}', '{\"Elie A Akl\",\"Gordon Guyatt\",\"Stephen D Walter\",\"Rita S Suri\",\"Reem A Mustafa\",\"Gihad E Nesrallah\",\"Robert M Lindsay\"}', -0.29897738369395577]\n",
      "['4/14/07', '7/4/14', '', 0.4285714285714286, 0.0, -0.4285714285714286]\n",
      "['1/14/07', '7/1/14', '', 0.4285714285714286, 0.0, -0.4285714285714286]\n",
      "['1/12/01', '1/1/12', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/10/01', '1/1/10', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['2/15/04', '4/2/15', '', 0.2857142857142857, 0.0, -0.2857142857142857]\n",
      "['1/14/11', '11/1/14', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/5/04', '4/1/05', '', 0.5, 0.0, -0.5]\n",
      "['1/12/01', '1/1/12', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/10/10', '10/1/10', '', 0.7142857142857143, 0.0, -0.7142857142857143]\n",
      "['1/15/01', '1/1/15', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/10/01', '1/1/10', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/12/01', '1/1/12', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/11/01', '1/1/11', '', 0.7142857142857143, 0.0, -0.7142857142857143]\n",
      "['9/15/10', '10/9/15', '', 0.2857142857142857, 0.0, -0.2857142857142857]\n",
      "['1/5/01', '1/1/05', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/11/01', '1/1/11', '', 0.7142857142857143, 0.0, -0.7142857142857143]\n",
      "['4/15/11', '11/4/15', '', 0.2857142857142857, 0.0, -0.2857142857142857]\n",
      "['1/12/01', '1/1/12', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/11/01', '1/1/11', '', 0.7142857142857143, 0.0, -0.7142857142857143]\n",
      "['1/11/01', '1/1/11', '', 0.7142857142857143, 0.0, -0.7142857142857143]\n",
      "['10/15/09', '9/10/15', '', 0.375, 0.0, -0.375]\n",
      "['{\"N_ria Lloberas\",\"Marcel la Franquesa\",\"Josep M Cruzado\",\"Josep M Griny_\",\"Ins Rama\",\"Gabriela Alperovich\",\"Immaculada Herrero-Fresneda\",\"Joan Torras\",\"Pepita Gimnez-Bonaf\"}', '{\"NÌ¼ria Lloberas\",\"Marcel la Franquesa\",\"Josep M Cruzado\",\"Josep M Grinyï¿½_\",\"InÌ©s Rama\",\"Gabriela Alperovich\",\"Immaculada Herrero-Fresneda\",\"Joan Torras\",\"Pepita GimÌ©nez-BonafÌ©\"}', '{\"Elie A Akl\",\"Gordon Guyatt\",\"Stephen D Walter\",\"Rita S Suri\",\"Reem A Mustafa\",\"Gihad E Nesrallah\",\"Robert M Lindsay\"}', 0.9719101123595506, 0.26436781609195403, -0.7075422962675966]\n",
      "['4/14/07', '7/4/14', '', -0.7158730158730159]\n",
      "['1/14/07', '7/1/14', '', -0.7158730158730159]\n",
      "['1/12/01', '1/1/12', '', -0.8968253968253969]\n",
      "['1/10/01', '1/1/10', '', -0.8968253968253969]\n",
      "['2/15/04', '4/2/15', '', -0.7158730158730159]\n",
      "['1/14/11', '11/1/14', '', -0.7936507936507936]\n",
      "['1/5/04', '4/1/05', '', -0.6944444444444443]\n",
      "['1/12/01', '1/1/12', '', -0.8968253968253969]\n",
      "['1/10/10', '10/1/10', '', -0.9523809523809524]\n",
      "['1/15/01', '1/1/15', '', -0.8968253968253969]\n",
      "['1/10/01', '1/1/10', '', -0.8968253968253969]\n",
      "['1/12/01', '1/1/12', '', -0.8968253968253969]\n",
      "['1/11/01', '1/1/11', '', -0.8968253968253969]\n",
      "['9/15/10', '10/9/15', '', -0.6309523809523809]\n",
      "['1/5/01', '1/1/05', '', -0.7777777777777777]\n",
      "['1/11/01', '1/1/11', '', -0.8968253968253969]\n",
      "['4/15/11', '11/4/15', '', -0.6309523809523809]\n",
      "['1/12/01', '1/1/12', '', -0.8968253968253969]\n",
      "['1/11/01', '1/1/11', '', -0.8968253968253969]\n",
      "['1/11/01', '1/1/11', '', -0.8968253968253969]\n",
      "['10/15/09', '9/10/15', '', -0.7023809523809524]\n",
      "['{\"N_ria Lloberas\",\"Marcel la Franquesa\",\"Josep M Cruzado\",\"Josep M Griny_\",\"Ins Rama\",\"Gabriela Alperovich\",\"Immaculada Herrero-Fresneda\",\"Joan Torras\",\"Pepita Gimnez-Bonaf\"}', '{\"NÌ¼ria Lloberas\",\"Marcel la Franquesa\",\"Josep M Cruzado\",\"Josep M Grinyï¿½_\",\"InÌ©s Rama\",\"Gabriela Alperovich\",\"Immaculada Herrero-Fresneda\",\"Joan Torras\",\"Pepita GimÌ©nez-BonafÌ©\"}', '{\"Elie A Akl\",\"Gordon Guyatt\",\"Stephen D Walter\",\"Rita S Suri\",\"Reem A Mustafa\",\"Gihad E Nesrallah\",\"Robert M Lindsay\"}', -0.29897738369395577]\n",
      "['4/14/07', '7/4/14', '', 0.4285714285714286, 0.0, -0.4285714285714286]\n",
      "['1/14/07', '7/1/14', '', 0.4285714285714286, 0.0, -0.4285714285714286]\n",
      "['1/12/01', '1/1/12', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/10/01', '1/1/10', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['2/15/04', '4/2/15', '', 0.2857142857142857, 0.0, -0.2857142857142857]\n",
      "['1/14/11', '11/1/14', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/5/04', '4/1/05', '', 0.5, 0.0, -0.5]\n",
      "['1/12/01', '1/1/12', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/10/10', '10/1/10', '', 0.7142857142857143, 0.0, -0.7142857142857143]\n",
      "['1/15/01', '1/1/15', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/10/01', '1/1/10', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/12/01', '1/1/12', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/11/01', '1/1/11', '', 0.7142857142857143, 0.0, -0.7142857142857143]\n",
      "['9/15/10', '10/9/15', '', 0.2857142857142857, 0.0, -0.2857142857142857]\n",
      "['1/5/01', '1/1/05', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/11/01', '1/1/11', '', 0.7142857142857143, 0.0, -0.7142857142857143]\n",
      "['4/15/11', '11/4/15', '', 0.2857142857142857, 0.0, -0.2857142857142857]\n",
      "['1/12/01', '1/1/12', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/11/01', '1/1/11', '', 0.7142857142857143, 0.0, -0.7142857142857143]\n",
      "['1/11/01', '1/1/11', '', 0.7142857142857143, 0.0, -0.7142857142857143]\n",
      "['10/15/09', '9/10/15', '', 0.375, 0.0, -0.375]\n",
      "['{\"N_ria Lloberas\",\"Marcel la Franquesa\",\"Josep M Cruzado\",\"Josep M Griny_\",\"Ins Rama\",\"Gabriela Alperovich\",\"Immaculada Herrero-Fresneda\",\"Joan Torras\",\"Pepita Gimnez-Bonaf\"}', '{\"NÌ¼ria Lloberas\",\"Marcel la Franquesa\",\"Josep M Cruzado\",\"Josep M Grinyï¿½_\",\"InÌ©s Rama\",\"Gabriela Alperovich\",\"Immaculada Herrero-Fresneda\",\"Joan Torras\",\"Pepita GimÌ©nez-BonafÌ©\"}', '{\"Elie A Akl\",\"Gordon Guyatt\",\"Stephen D Walter\",\"Rita S Suri\",\"Reem A Mustafa\",\"Gihad E Nesrallah\",\"Robert M Lindsay\"}', 0.9719101123595506, 0.26436781609195403, -0.7075422962675966]\n",
      "{'Combined Precision': 0.827, 'Combined Recall': 0.227, 'Combined F1': 0.356, 'PC R': 0.0, 'PC F': 22.0}\n",
      "\n",
      "{'Precision': 0.903, 'Recall': 0.248, 'F1': 0.389, 'Amount of fixed data errors': 259.0}\n",
      "{'Fuzzy JW Precision': 0.836, 'Fuzzy JW Recall': 0.229, 'Fuzzy JW F1': 0.36, 'PC R': 0.0, 'PC F': 22.0}\n",
      "{'Fuzzy ME Precision': 0.903, 'Fuzzy ME Recall': 0.248, 'Fuzzy ME F1': 0.389, 'PC R': 21.0, 'PC F': 1.0}\n",
      "{'Fuzzy LD Char Precision': 0.857, 'Fuzzy LD Char Recall': 0.235, 'Fuzzy LD Char F1': 0.369, 'PC R': 0.0, 'PC F': 22.0}\n",
      "{'Fuzzy Semantic Sentences Precision': 0.826, 'Fuzzy Semantic Sentences Recall': 0.227, 'Fuzzy Semantic Sentences F1': 0.356, 'PC R': 0.0, 'PC F': 22.0}\n",
      "\n",
      "{'Fuzzy JW Num Precision': 0.836, 'Fuzzy JW Num Recall': 0.229, 'Fuzzy JW Num F1': 0.36, 'PC R': 0.0, 'PC F': 22.0}\n",
      "{'Fuzzy ME Num Precision': 0.903, 'Fuzzy ME Num Recall': 0.248, 'Fuzzy ME Num F1': 0.389, 'PC R': 21.0, 'PC F': 1.0}\n",
      "{'Fuzzy LD Char Num Precision': 0.857, 'Fuzzy LD Char Num Recall': 0.235, 'Fuzzy LD Char Num F1': 0.369, 'PC R': 0.0, 'PC F': 22.0}\n",
      "{'Fuzzy Semantic Sentences Num Precision': 0.826, 'Fuzzy Semantic Sentences Num Recall': 0.227, 'Fuzzy Semantic Sentences Num F1': 0.356, 'PC R': 0.0, 'PC F': 22.0}\n"
     ]
    }
   ],
   "source": [
    "metric_dict_rayyan2 = {\n",
    "    \"clean_data_path\": \"../datasets/rayyan/clean.csv\",\n",
    "    \"dirty_data_path\": \"../datasets/rayyan/dirty.csv\",\n",
    "    \"corrected_data_path\": \"../datasets/rayyan/baran_repaired2.csv\",\n",
    "    \"str_attr\": [1, 2, 3, 4, 8, 10],\n",
    "    \"short_str_attr\": [2, 4, 8],\n",
    "    \"long_str_attr\": [1, 3, 10],\n",
    "    \"numer_attr\": [6, 7]\n",
    "    \n",
    "}\n",
    "\n",
    "m_rayyan2 = Metrics(metric_dict_rayyan2)\n",
    "\n",
    "m_rayyan2.print_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "121ca8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Combined Precision': 0.957, 'Combined Recall': 0.186, 'Combined F1': 0.312, 'PC R': 0.0, 'PC F': 0.0}\n",
      "\n",
      "{'Precision': 0.957, 'Recall': 0.186, 'F1': 0.312, 'Amount of fixed data errors': 184.0}\n",
      "{'Fuzzy JW Precision': 0.957, 'Fuzzy JW Recall': 0.186, 'Fuzzy JW F1': 0.312, 'PC R': 0.0, 'PC F': 0.0}\n",
      "{'Fuzzy ME Precision': 0.957, 'Fuzzy ME Recall': 0.186, 'Fuzzy ME F1': 0.312, 'PC R': 0.0, 'PC F': 0.0}\n",
      "{'Fuzzy LD Char Precision': 0.957, 'Fuzzy LD Char Recall': 0.186, 'Fuzzy LD Char F1': 0.312, 'PC R': 0.0, 'PC F': 0.0}\n",
      "{'Fuzzy Semantic Sentences Precision': 0.957, 'Fuzzy Semantic Sentences Recall': 0.186, 'Fuzzy Semantic Sentences F1': 0.312, 'PC R': 0.0, 'PC F': 0.0}\n",
      "\n",
      "{'Fuzzy JW Num Precision': 0.957, 'Fuzzy JW Num Recall': 0.186, 'Fuzzy JW Num F1': 0.312, 'PC R': 0.0, 'PC F': 0.0}\n",
      "{'Fuzzy ME Num Precision': 0.957, 'Fuzzy ME Num Recall': 0.186, 'Fuzzy ME Num F1': 0.312, 'PC R': 0.0, 'PC F': 0.0}\n",
      "{'Fuzzy LD Char Num Precision': 0.957, 'Fuzzy LD Char Num Recall': 0.186, 'Fuzzy LD Char Num F1': 0.312, 'PC R': 0.0, 'PC F': 0.0}\n",
      "{'Fuzzy Semantic Sentences Num Precision': 0.957, 'Fuzzy Semantic Sentences Num Recall': 0.186, 'Fuzzy Semantic Sentences Num F1': 0.312, 'PC R': 0.0, 'PC F': 0.0}\n"
     ]
    }
   ],
   "source": [
    "metric_dict_rayyan3 = {\n",
    "    \"clean_data_path\": \"../datasets/rayyan/clean.csv\",\n",
    "    \"dirty_data_path\": \"../datasets/rayyan/dirty.csv\",\n",
    "    \"corrected_data_path\": \"../datasets/rayyan/baran_repaired3.csv\",\n",
    "    \"str_attr\": [1, 2, 3, 4, 8, 10],\n",
    "    \"short_str_attr\": [2, 4, 8],\n",
    "    \"long_str_attr\": [1, 3, 10],\n",
    "    \"numer_attr\": [6, 7]\n",
    "    \n",
    "}\n",
    "\n",
    "m_rayyan3 = Metrics(metric_dict_rayyan3)\n",
    "\n",
    "m_rayyan3.print_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36c36836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1/5/01', '1/1/05', '', -0.7777777777777777]\n",
      "['1/13/12', '12/1/13', '7/15/14', -0.17857142857142871]\n",
      "['1/14/07', '7/1/14', '8/2/06', -0.07301587301587298]\n",
      "['1/3/06', '6/1/03', '11/1/96', 0.05158730158730174]\n",
      "['1/14/06', '6/1/14', '9/23/14', -0.19206349206349205]\n",
      "['1/11/04', '4/1/11', '10/24/14', -0.002777777777777768]\n",
      "['1/2/01', '1/1/02', '1/1/01', 0.11111111111111127]\n",
      "['1/7/01', '1/1/07', '', -0.7777777777777777]\n",
      "['1/7/01', '1/1/07', '', -0.7777777777777777]\n",
      "['1/9/01', '1/1/09', '', -0.7777777777777777]\n",
      "['1/8/01', '1/1/08', '11/4/15', -0.03174603174603163]\n",
      "['1/13/01', '1/1/13', '', -0.8968253968253969]\n",
      "['1/9/03', '3/1/09', '6/1/09', 0.0]\n",
      "['1/6/01', '1/1/06', '2/16/10', 0.0634920634920636]\n",
      "['1/7/10', '10/1/07', '10/1/15', -0.11984126984126986]\n",
      "['1/13/01', '1/1/13', '', -0.8968253968253969]\n",
      "['3/15/04', '4/3/15', '5/11/07', -0.0015873015873015817]\n",
      "['1/14/11', '11/1/14', '8/2/06', -0.25396825396825395]\n",
      "['1/13/01', '1/1/13', '10/1/08', -0.08730158730158721]\n",
      "['1/1/06', '6/1/01', '11/1/98', -0.03174603174603163]\n",
      "['1/11/11', '11/1/11', '2/16/10', -0.4047619047619049]\n",
      "['1/14/12', '12/1/14', '9/1/10', -0.23015873015873023]\n",
      "['1/15/02', '2/1/15', '7/24/15', -0.19206349206349205]\n",
      "['1/10/03', '3/1/10', '', -0.7158730158730159]\n",
      "['1/1/12', '12/1/01', '8/1/65', -0.2936507936507937]\n",
      "['1/13/01', '1/1/13', '', -0.8968253968253969]\n",
      "['1/15/01', '1/1/15', '', -0.8968253968253969]\n",
      "['1/13/01', '1/1/13', '', -0.8968253968253969]\n",
      "['1/13/01', '1/1/13', '2/16/10', -0.22063492063492074]\n",
      "['1/5/01', '1/1/05', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/13/12', '12/1/13', '7/15/14', 0.5714285714285714, 0.5714285714285714, 0.0]\n",
      "['1/14/07', '7/1/14', '8/2/06', 0.4285714285714286, 0.4285714285714286, 0.0]\n",
      "['1/3/06', '6/1/03', '11/1/96', 0.5, 0.5714285714285714, 0.0714285714285714]\n",
      "['1/14/06', '6/1/14', '9/23/14', 0.4285714285714286, 0.2857142857142857, -0.1428571428571429]\n",
      "['1/11/04', '4/1/11', '10/24/14', 0.4285714285714286, 0.5, 0.0714285714285714]\n",
      "['1/2/01', '1/1/02', '1/1/01', 0.6666666666666667, 0.8333333333333334, 0.16666666666666663]\n",
      "['1/7/01', '1/1/07', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/7/01', '1/1/07', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/9/01', '1/1/09', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/8/01', '1/1/08', '11/4/15', 0.6666666666666667, 0.4285714285714286, -0.23809523809523814]\n",
      "['1/13/01', '1/1/13', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/9/03', '3/1/09', '6/1/09', 0.5, 0.5, 0.0]\n",
      "['1/6/01', '1/1/06', '2/16/10', 0.6666666666666667, 0.4285714285714286, -0.23809523809523814]\n",
      "['1/7/10', '10/1/07', '10/1/15', 0.4285714285714286, 0.5714285714285714, 0.1428571428571428]\n",
      "['1/13/01', '1/1/13', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['3/15/04', '4/3/15', '5/11/07', 0.2857142857142857, 0.5714285714285714, 0.2857142857142857]\n",
      "['1/14/11', '11/1/14', '8/2/06', 0.5714285714285714, 0.2857142857142857, -0.2857142857142857]\n",
      "['1/13/01', '1/1/13', '10/1/08', 0.5714285714285714, 0.5714285714285714, 0.0]\n",
      "['1/1/06', '6/1/01', '11/1/98', 0.6666666666666667, 0.5714285714285714, -0.09523809523809534]\n",
      "['1/11/11', '11/1/11', '2/16/10', 0.7142857142857143, 0.5714285714285714, -0.1428571428571429]\n",
      "['1/14/12', '12/1/14', '9/1/10', 0.5714285714285714, 0.5714285714285714, 0.0]\n",
      "['1/15/02', '2/1/15', '7/24/15', 0.4285714285714286, 0.2857142857142857, -0.1428571428571429]\n",
      "['1/10/03', '3/1/10', '', 0.4285714285714286, 0.0, -0.4285714285714286]\n",
      "['1/1/12', '12/1/01', '8/1/65', 0.5714285714285714, 0.5, -0.0714285714285714]\n",
      "['1/13/01', '1/1/13', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/15/01', '1/1/15', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/13/01', '1/1/13', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/13/01', '1/1/13', '2/16/10', 0.5714285714285714, 0.4285714285714286, -0.1428571428571428]\n",
      "['1/5/01', '1/1/05', '', -0.7777777777777777]\n",
      "['1/13/12', '12/1/13', '7/15/14', -0.17857142857142871]\n",
      "['1/14/07', '7/1/14', '8/2/06', -0.07301587301587298]\n",
      "['1/3/06', '6/1/03', '11/1/96', 0.05158730158730174]\n",
      "['1/14/06', '6/1/14', '9/23/14', -0.19206349206349205]\n",
      "['1/11/04', '4/1/11', '10/24/14', -0.002777777777777768]\n",
      "['1/2/01', '1/1/02', '1/1/01', 0.11111111111111127]\n",
      "['1/7/01', '1/1/07', '', -0.7777777777777777]\n",
      "['1/7/01', '1/1/07', '', -0.7777777777777777]\n",
      "['1/9/01', '1/1/09', '', -0.7777777777777777]\n",
      "['1/8/01', '1/1/08', '11/4/15', -0.03174603174603163]\n",
      "['1/13/01', '1/1/13', '', -0.8968253968253969]\n",
      "['1/9/03', '3/1/09', '6/1/09', 0.0]\n",
      "['1/6/01', '1/1/06', '2/16/10', 0.0634920634920636]\n",
      "['1/7/10', '10/1/07', '10/1/15', -0.11984126984126986]\n",
      "['1/13/01', '1/1/13', '', -0.8968253968253969]\n",
      "['3/15/04', '4/3/15', '5/11/07', -0.0015873015873015817]\n",
      "['1/14/11', '11/1/14', '8/2/06', -0.25396825396825395]\n",
      "['1/13/01', '1/1/13', '10/1/08', -0.08730158730158721]\n",
      "['1/1/06', '6/1/01', '11/1/98', -0.03174603174603163]\n",
      "['1/11/11', '11/1/11', '2/16/10', -0.4047619047619049]\n",
      "['1/14/12', '12/1/14', '9/1/10', -0.23015873015873023]\n",
      "['1/15/02', '2/1/15', '7/24/15', -0.19206349206349205]\n",
      "['1/10/03', '3/1/10', '', -0.7158730158730159]\n",
      "['1/1/12', '12/1/01', '8/1/65', -0.2936507936507937]\n",
      "['1/13/01', '1/1/13', '', -0.8968253968253969]\n",
      "['1/15/01', '1/1/15', '', -0.8968253968253969]\n",
      "['1/13/01', '1/1/13', '', -0.8968253968253969]\n",
      "['1/13/01', '1/1/13', '2/16/10', -0.22063492063492074]\n",
      "['1/5/01', '1/1/05', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/13/12', '12/1/13', '7/15/14', 0.5714285714285714, 0.5714285714285714, 0.0]\n",
      "['1/14/07', '7/1/14', '8/2/06', 0.4285714285714286, 0.4285714285714286, 0.0]\n",
      "['1/3/06', '6/1/03', '11/1/96', 0.5, 0.5714285714285714, 0.0714285714285714]\n",
      "['1/14/06', '6/1/14', '9/23/14', 0.4285714285714286, 0.2857142857142857, -0.1428571428571429]\n",
      "['1/11/04', '4/1/11', '10/24/14', 0.4285714285714286, 0.5, 0.0714285714285714]\n",
      "['1/2/01', '1/1/02', '1/1/01', 0.6666666666666667, 0.8333333333333334, 0.16666666666666663]\n",
      "['1/7/01', '1/1/07', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/7/01', '1/1/07', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/9/01', '1/1/09', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/8/01', '1/1/08', '11/4/15', 0.6666666666666667, 0.4285714285714286, -0.23809523809523814]\n",
      "['1/13/01', '1/1/13', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/9/03', '3/1/09', '6/1/09', 0.5, 0.5, 0.0]\n",
      "['1/6/01', '1/1/06', '2/16/10', 0.6666666666666667, 0.4285714285714286, -0.23809523809523814]\n",
      "['1/7/10', '10/1/07', '10/1/15', 0.4285714285714286, 0.5714285714285714, 0.1428571428571428]\n",
      "['1/13/01', '1/1/13', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['3/15/04', '4/3/15', '5/11/07', 0.2857142857142857, 0.5714285714285714, 0.2857142857142857]\n",
      "['1/14/11', '11/1/14', '8/2/06', 0.5714285714285714, 0.2857142857142857, -0.2857142857142857]\n",
      "['1/13/01', '1/1/13', '10/1/08', 0.5714285714285714, 0.5714285714285714, 0.0]\n",
      "['1/1/06', '6/1/01', '11/1/98', 0.6666666666666667, 0.5714285714285714, -0.09523809523809534]\n",
      "['1/11/11', '11/1/11', '2/16/10', 0.7142857142857143, 0.5714285714285714, -0.1428571428571429]\n",
      "['1/14/12', '12/1/14', '9/1/10', 0.5714285714285714, 0.5714285714285714, 0.0]\n",
      "['1/15/02', '2/1/15', '7/24/15', 0.4285714285714286, 0.2857142857142857, -0.1428571428571429]\n",
      "['1/10/03', '3/1/10', '', 0.4285714285714286, 0.0, -0.4285714285714286]\n",
      "['1/1/12', '12/1/01', '8/1/65', 0.5714285714285714, 0.5, -0.0714285714285714]\n",
      "['1/13/01', '1/1/13', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/15/01', '1/1/15', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/13/01', '1/1/13', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/13/01', '1/1/13', '2/16/10', 0.5714285714285714, 0.4285714285714286, -0.1428571428571428]\n",
      "{'Combined Precision': 0.746, 'Combined Recall': 0.17, 'Combined F1': 0.277, 'PC R': 4.0, 'PC F': 25.0}\n",
      "\n",
      "{'Precision': 0.805, 'Recall': 0.183, 'F1': 0.299, 'Amount of fixed data errors': 215.0}\n",
      "{'Fuzzy JW Precision': 0.756, 'Fuzzy JW Recall': 0.172, 'Fuzzy JW F1': 0.281, 'PC R': 4.0, 'PC F': 25.0}\n",
      "{'Fuzzy ME Precision': 0.806, 'Fuzzy ME Recall': 0.183, 'Fuzzy ME F1': 0.299, 'PC R': 25.0, 'PC F': 4.0}\n",
      "{'Fuzzy LD Char Precision': 0.773, 'Fuzzy LD Char Recall': 0.176, 'Fuzzy LD Char F1': 0.287, 'PC R': 10.0, 'PC F': 19.0}\n",
      "{'Fuzzy Semantic Sentences Precision': 0.743, 'Fuzzy Semantic Sentences Recall': 0.169, 'Fuzzy Semantic Sentences F1': 0.275, 'PC R': 2.0, 'PC F': 27.0}\n",
      "\n",
      "{'Fuzzy JW Num Precision': 0.756, 'Fuzzy JW Num Recall': 0.172, 'Fuzzy JW Num F1': 0.281, 'PC R': 4.0, 'PC F': 25.0}\n",
      "{'Fuzzy ME Num Precision': 0.806, 'Fuzzy ME Num Recall': 0.183, 'Fuzzy ME Num F1': 0.299, 'PC R': 25.0, 'PC F': 4.0}\n",
      "{'Fuzzy LD Char Num Precision': 0.773, 'Fuzzy LD Char Num Recall': 0.176, 'Fuzzy LD Char Num F1': 0.287, 'PC R': 10.0, 'PC F': 19.0}\n",
      "{'Fuzzy Semantic Sentences Num Precision': 0.743, 'Fuzzy Semantic Sentences Num Recall': 0.169, 'Fuzzy Semantic Sentences Num F1': 0.275, 'PC R': 2.0, 'PC F': 27.0}\n"
     ]
    }
   ],
   "source": [
    "metric_dict_rayyan4 = {\n",
    "    \"clean_data_path\": \"../datasets/rayyan/clean.csv\",\n",
    "    \"dirty_data_path\": \"../datasets/rayyan/dirty.csv\",\n",
    "    \"corrected_data_path\": \"../datasets/rayyan/baran_repaired4.csv\",\n",
    "    \"str_attr\": [1, 2, 3, 4, 8, 10],\n",
    "    \"short_str_attr\": [2, 4, 8],\n",
    "    \"long_str_attr\": [1, 3, 10],\n",
    "    \"numer_attr\": [6, 7]\n",
    "    \n",
    "}\n",
    "\n",
    "m_rayyan4 = Metrics(metric_dict_rayyan4)\n",
    "\n",
    "m_rayyan4.print_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37260eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1/4/01', '1/1/04', '', -0.7777777777777777]\n",
      "['1/7/01', '1/1/07', '', -0.7777777777777777]\n",
      "['1/14/01', '1/1/14', '', -0.8968253968253969]\n",
      "['1/12/01', '1/1/12', '', -0.8968253968253969]\n",
      "['1/10/01', '1/1/10', '', -0.8968253968253969]\n",
      "['1/7/01', '1/1/07', '', -0.7777777777777777]\n",
      "['1/4/01', '1/1/04', '', -0.7777777777777777]\n",
      "['1/11/01', '1/1/11', '', -0.8968253968253969]\n",
      "['1/13/01', '1/1/13', '', -0.8968253968253969]\n",
      "['{\"Michela Miani\",\"Decio L Eizirik\",\"Laurence Ladrire\",\"Maikel L Colli\",\"Miriam Cnop\"}', '{\"Michela Miani\",\"Decio L Eizirik\",\"Laurence LadriÌ¬re\",\"Maikel L Colli\",\"Miriam Cnop\"}', '{\"Thomas Scherer\",\"Jose M Garcia\",\"Victor Papusha\",\"Ji-an Chen\",\"Mark Asnicar\",\"Joanna Smiechowska\",\"Bobby Guillory\",\"Christoph Buettner\",\"Roy G Smith\",\"Anriada Nassif\"}', -0.4010353240559721]\n",
      "['1/4/01', '1/1/04', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/7/01', '1/1/07', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/14/01', '1/1/14', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/12/01', '1/1/12', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/10/01', '1/1/10', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/7/01', '1/1/07', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/4/01', '1/1/04', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/11/01', '1/1/11', '', 0.7142857142857143, 0.0, -0.7142857142857143]\n",
      "['1/13/01', '1/1/13', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['{\"Michela Miani\",\"Decio L Eizirik\",\"Laurence Ladrire\",\"Maikel L Colli\",\"Miriam Cnop\"}', '{\"Michela Miani\",\"Decio L Eizirik\",\"Laurence LadriÌ¬re\",\"Maikel L Colli\",\"Miriam Cnop\"}', '{\"Thomas Scherer\",\"Jose M Garcia\",\"Victor Papusha\",\"Ji-an Chen\",\"Mark Asnicar\",\"Joanna Smiechowska\",\"Bobby Guillory\",\"Christoph Buettner\",\"Roy G Smith\",\"Anriada Nassif\"}', 0.9883720930232558, 0.24852071005917165, -0.7398513829640841]\n",
      "['1/4/01', '1/1/04', '', -0.7777777777777777]\n",
      "['1/7/01', '1/1/07', '', -0.7777777777777777]\n",
      "['1/14/01', '1/1/14', '', -0.8968253968253969]\n",
      "['1/12/01', '1/1/12', '', -0.8968253968253969]\n",
      "['1/10/01', '1/1/10', '', -0.8968253968253969]\n",
      "['1/7/01', '1/1/07', '', -0.7777777777777777]\n",
      "['1/4/01', '1/1/04', '', -0.7777777777777777]\n",
      "['1/11/01', '1/1/11', '', -0.8968253968253969]\n",
      "['1/13/01', '1/1/13', '', -0.8968253968253969]\n",
      "['{\"Michela Miani\",\"Decio L Eizirik\",\"Laurence Ladrire\",\"Maikel L Colli\",\"Miriam Cnop\"}', '{\"Michela Miani\",\"Decio L Eizirik\",\"Laurence LadriÌ¬re\",\"Maikel L Colli\",\"Miriam Cnop\"}', '{\"Thomas Scherer\",\"Jose M Garcia\",\"Victor Papusha\",\"Ji-an Chen\",\"Mark Asnicar\",\"Joanna Smiechowska\",\"Bobby Guillory\",\"Christoph Buettner\",\"Roy G Smith\",\"Anriada Nassif\"}', -0.4010353240559721]\n",
      "['1/4/01', '1/1/04', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/7/01', '1/1/07', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/14/01', '1/1/14', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/12/01', '1/1/12', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/10/01', '1/1/10', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['1/7/01', '1/1/07', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/4/01', '1/1/04', '', 0.6666666666666667, 0.0, -0.6666666666666667]\n",
      "['1/11/01', '1/1/11', '', 0.7142857142857143, 0.0, -0.7142857142857143]\n",
      "['1/13/01', '1/1/13', '', 0.5714285714285714, 0.0, -0.5714285714285714]\n",
      "['{\"Michela Miani\",\"Decio L Eizirik\",\"Laurence Ladrire\",\"Maikel L Colli\",\"Miriam Cnop\"}', '{\"Michela Miani\",\"Decio L Eizirik\",\"Laurence LadriÌ¬re\",\"Maikel L Colli\",\"Miriam Cnop\"}', '{\"Thomas Scherer\",\"Jose M Garcia\",\"Victor Papusha\",\"Ji-an Chen\",\"Mark Asnicar\",\"Joanna Smiechowska\",\"Bobby Guillory\",\"Christoph Buettner\",\"Roy G Smith\",\"Anriada Nassif\"}', 0.9883720930232558, 0.24852071005917165, -0.7398513829640841]\n",
      "{'Combined Precision': 0.815, 'Combined Recall': 0.136, 'Combined F1': 0.234, 'PC R': 0.0, 'PC F': 10.0}\n",
      "\n",
      "{'Precision': 0.873, 'Recall': 0.146, 'F1': 0.25, 'Amount of fixed data errors': 158.0}\n",
      "{'Fuzzy JW Precision': 0.823, 'Fuzzy JW Recall': 0.138, 'Fuzzy JW F1': 0.236, 'PC R': 0.0, 'PC F': 10.0}\n",
      "{'Fuzzy ME Precision': 0.873, 'Fuzzy ME Recall': 0.146, 'Fuzzy ME F1': 0.25, 'PC R': 9.0, 'PC F': 1.0}\n",
      "{'Fuzzy LD Char Precision': 0.833, 'Fuzzy LD Char Recall': 0.139, 'Fuzzy LD Char F1': 0.239, 'PC R': 0.0, 'PC F': 10.0}\n",
      "{'Fuzzy Semantic Sentences Precision': 0.815, 'Fuzzy Semantic Sentences Recall': 0.136, 'Fuzzy Semantic Sentences F1': 0.234, 'PC R': 0.0, 'PC F': 10.0}\n",
      "\n",
      "{'Fuzzy JW Num Precision': 0.823, 'Fuzzy JW Num Recall': 0.138, 'Fuzzy JW Num F1': 0.236, 'PC R': 0.0, 'PC F': 10.0}\n",
      "{'Fuzzy ME Num Precision': 0.873, 'Fuzzy ME Num Recall': 0.146, 'Fuzzy ME Num F1': 0.25, 'PC R': 9.0, 'PC F': 1.0}\n",
      "{'Fuzzy LD Char Num Precision': 0.833, 'Fuzzy LD Char Num Recall': 0.139, 'Fuzzy LD Char Num F1': 0.239, 'PC R': 0.0, 'PC F': 10.0}\n",
      "{'Fuzzy Semantic Sentences Num Precision': 0.815, 'Fuzzy Semantic Sentences Num Recall': 0.136, 'Fuzzy Semantic Sentences Num F1': 0.234, 'PC R': 0.0, 'PC F': 10.0}\n"
     ]
    }
   ],
   "source": [
    "metric_dict_rayyan5 = {\n",
    "    \"clean_data_path\": \"../datasets/rayyan/clean.csv\",\n",
    "    \"dirty_data_path\": \"../datasets/rayyan/dirty.csv\",\n",
    "    \"corrected_data_path\": \"../datasets/rayyan/baran_repaired5.csv\",\n",
    "    \"str_attr\": [1, 2, 3, 4, 8, 10],\n",
    "    \"short_str_attr\": [2, 4, 8],\n",
    "    \"long_str_attr\": [1, 3, 10],\n",
    "    \"numer_attr\": [6, 7]\n",
    "    \n",
    "}\n",
    "\n",
    "m_rayyan5 = Metrics(metric_dict_rayyan5)\n",
    "\n",
    "m_rayyan5.print_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f875940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of 5 runs\n",
      "{'Combined Precision': 0.823, 'Combined Recall': 0.19, 'Combined F1': 0.307, 'PC R': 0.0, 'PC F': 13.0}\n",
      "{'Precision': 0.868, 'Recall': 0.201, 'F1': 0.325, 'Amount of fixed data errors': 285.0}\n",
      "{'Fuzzy JW Precision': 0.83, 'Fuzzy JW Recall': 0.192, 'Fuzzy JW F1': 0.31, 'PC R': 0.0, 'PC F': 13.0}\n",
      "{'Fuzzy ME Precision': 0.868, 'Fuzzy ME Recall': 0.201, 'Fuzzy ME F1': 0.325, 'PC R': 10.0, 'PC F': 3.0}\n",
      "{'Fuzzy LD Char Precision': 0.84, 'Fuzzy LD Char Recall': 0.194, 'Fuzzy LD Char F1': 0.314, 'PC R': 1.0, 'PC F': 12.0}\n",
      "{'Fuzzy Semantic Sentences Precision': 0.822, 'Fuzzy Semantic Sentences Recall': 0.19, 'Fuzzy Semantic Sentences F1': 0.307, 'PC R': 0.0, 'PC F': 13.0}\n",
      "\n",
      "{'Fuzzy JW Num Precision': 0.83, 'Fuzzy JW Num Recall': 0.192, 'Fuzzy JW Num F1': 0.31, 'PC R': 0.0, 'PC F': 13.0}\n",
      "{'Fuzzy ME Num Precision': 0.868, 'Fuzzy ME Num Recall': 0.201, 'Fuzzy ME Num F1': 0.325, 'PC R': 10.0, 'PC F': 3.0}\n",
      "{'Fuzzy LD Char Num Precision': 0.84, 'Fuzzy LD Char Num Recall': 0.194, 'Fuzzy LD Char Num F1': 0.314, 'PC R': 1.0, 'PC F': 12.0}\n",
      "{'Fuzzy Semantic Sentences Num Precision': 0.822, 'Fuzzy Semantic Sentences Num Recall': 0.19, 'Fuzzy Semantic Sentences Num F1': 0.307, 'PC R': 0.0, 'PC F': 13.0}\n"
     ]
    }
   ],
   "source": [
    "def calc_avg(m_dict_list):\n",
    "    \n",
    "    result = m_dict_list[0].copy()\n",
    "    \n",
    "    \n",
    "    for i in range(0,3):\n",
    "        if len(m_dict_list[i]) > 1:\n",
    "            v = 0\n",
    "            for j in range(0, len(m_dict_list)):\n",
    "                v += list(m_dict_list[j].values())[i]\n",
    "            result[list(m_dict_list[0].keys())[i]] = round(v/len(m_dict_list),3)\n",
    "\n",
    "    print(result)\n",
    "    #return result\n",
    "    \n",
    "print(\"Average of 5 runs\")\n",
    "calc_avg([m_rayyan1.combined_metric, m_rayyan2.combined_metric, m_rayyan3.combined_metric, m_rayyan4.combined_metric, m_rayyan5.combined_metric])\n",
    "\n",
    "calc_avg([m_rayyan1.standard_metric, m_rayyan2.standard_metric, m_rayyan3.standard_metric, m_rayyan4.standard_metric, m_rayyan5.standard_metric])\n",
    "#calc_avg([m_rayyan1.fuzzy_metric, m_rayyan2.fuzzy_metric, m_rayyan3.fuzzy_metric, m_rayyan4.fuzzy_metric, m_rayyan5.fuzzy_metric])\n",
    "#calc_avg([m_rayyan1.fuzzy_alt_metric, m_rayyan2.fuzzy_alt_metric, m_rayyan3.fuzzy_alt_metric, m_rayyan4.fuzzy_alt_metric, m_rayyan5.fuzzy_alt_metric])\n",
    "calc_avg([m_rayyan1.fuzzy_jw, m_rayyan2.fuzzy_jw, m_rayyan3.fuzzy_jw, m_rayyan4.fuzzy_jw, m_rayyan5.fuzzy_jw])\n",
    "calc_avg([m_rayyan1.fuzzy_me, m_rayyan2.fuzzy_me, m_rayyan3.fuzzy_me, m_rayyan4.fuzzy_me, m_rayyan5.fuzzy_me])\n",
    "#calc_avg([m_rayyan1.fuzzy_ld, m_rayyan2.fuzzy_ld, m_rayyan3.fuzzy_ld, m_rayyan4.fuzzy_ld, m_rayyan5.fuzzy_ld])\n",
    "#calc_avg([m_rayyan1.fuzzy_ld_words, m_rayyan2.fuzzy_ld_words, m_rayyan3.fuzzy_ld_words, m_rayyan4.fuzzy_ld_words, m_rayyan5.fuzzy_ld_words])\n",
    "calc_avg([m_rayyan1.fuzzy_ld_char, m_rayyan2.fuzzy_ld_char, m_rayyan3.fuzzy_ld_char, m_rayyan4.fuzzy_ld_char, m_rayyan5.fuzzy_ld_char])\n",
    "calc_avg([m_rayyan1.fuzzy_semantics_sentences, m_rayyan2.fuzzy_semantics_sentences, m_rayyan3.fuzzy_semantics_sentences, m_rayyan4.fuzzy_semantics_sentences, m_rayyan5.fuzzy_semantics_sentences])\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "#calc_avg([m_rayyan1.fuzzy_num_metric, m_rayyan2.fuzzy_num_metric, m_rayyan3.fuzzy_num_metric, m_rayyan4.fuzzy_num_metric, m_rayyan5.fuzzy_num_metric])\n",
    "#calc_avg([m_rayyan1.fuzzy_alt_num_metric, m_rayyan2.fuzzy_alt_num_metric, m_rayyan3.fuzzy_alt_num_metric, m_rayyan4.fuzzy_alt_num_metric, m_rayyan5.fuzzy_alt_num_metric])\n",
    "calc_avg([m_rayyan1.fuzzy_jw_num, m_rayyan2.fuzzy_jw_num, m_rayyan3.fuzzy_jw_num, m_rayyan4.fuzzy_jw_num, m_rayyan5.fuzzy_jw_num])\n",
    "calc_avg([m_rayyan1.fuzzy_me_num, m_rayyan2.fuzzy_me_num, m_rayyan3.fuzzy_me_num, m_rayyan4.fuzzy_me_num, m_rayyan5.fuzzy_me_num])\n",
    "#calc_avg([m_rayyan1.fuzzy_ld_num, m_rayyan2.fuzzy_ld_num, m_rayyan3.fuzzy_ld_num, m_rayyan4.fuzzy_ld_num, m_rayyan5.fuzzy_ld_num])\n",
    "#calc_avg([m_rayyan1.fuzzy_ld_words_num, m_rayyan2.fuzzy_ld_words_num, m_rayyan3.fuzzy_ld_words_num, m_rayyan4.fuzzy_ld_words_num, m_rayyan5.fuzzy_ld_words_num])\n",
    "calc_avg([m_rayyan1.fuzzy_ld_char_num, m_rayyan2.fuzzy_ld_char_num, m_rayyan3.fuzzy_ld_char_num, m_rayyan4.fuzzy_ld_char_num, m_rayyan5.fuzzy_ld_char_num])\n",
    "calc_avg([m_rayyan1.fuzzy_semantics_sentences_num, m_rayyan2.fuzzy_semantics_sentences_num, m_rayyan3.fuzzy_semantics_sentences_num, m_rayyan4.fuzzy_semantics_sentences_num, m_rayyan5.fuzzy_semantics_sentences_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8ee5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
